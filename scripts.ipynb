{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def min_max_normalize(arr):\n",
    "    if not arr:  # Check if the array is empty\n",
    "        return []\n",
    "\n",
    "    min_val = min(arr)\n",
    "    max_val = max(arr)\n",
    "    \n",
    "    if min_val == max_val:  # Check if all values in array are the same\n",
    "        return [0] * len(arr)\n",
    "\n",
    "    # Apply min-max normalization and round to 3 decimal places\n",
    "    return [round((x - min_val) / (max_val - min_val), 3) for x in arr]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0, 1.0, 0.5]\n"
     ]
    }
   ],
   "source": [
    "#example of use\n",
    "arr = [2,12,7]\n",
    "print(min_max_normalize(arr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kmean clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def euclidean_distance(point1, point2):\n",
    "    \"\"\"Calculate the Euclidean distance between two points.\"\"\"\n",
    "    return np.sqrt(np.sum((np.array(point1) - np.array(point2)) ** 2))\n",
    "\n",
    "def classify_points(points, centroids):\n",
    "    \"\"\"Classify points based on the closest centroid using K-means clustering.\n",
    "    In case of equidistant points, choose the cluster with the lowest index.\n",
    "\n",
    "    Args:\n",
    "    points (list of lists): The points to classify.\n",
    "    centroids (list of lists): The centroids to use for classification.\n",
    "\n",
    "    Returns:\n",
    "    list: A list of indices indicating the closest centroid for each point.\n",
    "    \"\"\"\n",
    "    classifications = []\n",
    "    for point in points:\n",
    "        distances = [euclidean_distance(point, centroid) for centroid in centroids]\n",
    "        min_distance = min(distances)\n",
    "        # Find indices of all centroids that have the minimum distance\n",
    "        min_indices = [i for i, d in enumerate(distances) if d == min_distance]\n",
    "        # Choose the centroid with the lowest index among those\n",
    "        classification = min(min_indices)\n",
    "        classifications.append(classification)\n",
    "    return classifications\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Point (3, 1, 0, 4) is classified to centroid (1.5, 2.5, 1, 2)\n",
      "Point (1, 3, 4.5, 5) is classified to centroid (3, 5, 4, 4.5)\n",
      "Point (6, 3, 2, 0) is classified to centroid (2, 3.5, 1, 0)\n"
     ]
    }
   ],
   "source": [
    "points = [(3,1,0,4), (1,3,4.5,5), (6,3,2,0)]\n",
    "centroids = [(3,5,4,4.5), (1.5,2.5,1,2),(2,3.5,1,0)]\n",
    "\n",
    "# Classify the points\n",
    "classifications = classify_points(points, centroids)\n",
    "\n",
    "# Output the classification result\n",
    "for point, classification in zip(points, classifications):\n",
    "    print(f\"Point {point} is classified to centroid {centroids[classification]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dirichlet smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probability of term5 in doc1 is 0.149\n",
      "Probability of term4 in the collection is 0.318\n",
      "Probability of term2 in doc3 is 0.074\n",
      "Term with lowest probability in doc2 is term3\n",
      "Document with highest score is doc3\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Term-document matrix as provided\n",
    "term_document_matrix = np.array([\n",
    "    [1, 1, 2, 1],  # term1\n",
    "    [0, 2, 0, 1],  # term2\n",
    "    [2, 0, 1, 0],  # term3\n",
    "    [4, 0, 1, 2],  # term4\n",
    "    [1, 2, 1, 0]   # term5\n",
    "])\n",
    "\n",
    "# Dirichlet smoothing parameter\n",
    "mu = 6\n",
    "\n",
    "# Calculate the total number of words in each document\n",
    "doc_lengths = term_document_matrix.sum(axis=0)\n",
    "\n",
    "# Calculate the total count of each term in the collection (sum over all documents)\n",
    "term_frequencies = term_document_matrix.sum(axis=1)\n",
    "\n",
    "# Total number of words in the collection\n",
    "collection_length = term_frequencies.sum()\n",
    "\n",
    "# Calculate the probability of each term in the collection\n",
    "term_prob_collection = term_frequencies / collection_length\n",
    "\n",
    "# Function to calculate the smoothed probability of a term in a document\n",
    "def dirichlet_smoothed_probability(term_idx, doc_idx, term_document_matrix, mu, term_prob_collection):\n",
    "    term_count = term_document_matrix[term_idx, doc_idx]\n",
    "    doc_length = doc_lengths[doc_idx]\n",
    "    prob_term_collection = term_prob_collection[term_idx]\n",
    "    return (term_count + mu * prob_term_collection) / (doc_length + mu)\n",
    "\n",
    "# Calculate the probability of term5 in the empirical language model of doc1\n",
    "prob_term5_doc1 = dirichlet_smoothed_probability(4, 0, term_document_matrix, mu, term_prob_collection)\n",
    "\n",
    "# Calculate the probability of term4 in the background language model (collection)\n",
    "prob_term4_collection = term_prob_collection[3]\n",
    "\n",
    "# Calculate the probability of term2 in the smoothed language model of doc3\n",
    "prob_term2_doc3 = dirichlet_smoothed_probability(1, 2, term_document_matrix, mu, term_prob_collection)\n",
    "\n",
    "# Calculate the smoothed probabilities for all terms in doc2 for finding the term with lowest probability\n",
    "prob_terms_doc2 = [dirichlet_smoothed_probability(term_idx, 1, term_document_matrix, mu, term_prob_collection) for term_idx in range(term_document_matrix.shape[0])]\n",
    "lowest_prob_term_doc2 = np.argmin(prob_terms_doc2) + 1 # adding 1 to match term numbering\n",
    "\n",
    "# Function to calculate the score of a document for a given query\n",
    "def document_score(query_terms, doc_idx, term_document_matrix, mu, term_prob_collection):\n",
    "    score = 1\n",
    "    for term in query_terms:\n",
    "        term_idx = int(term[-1]) - 1  # Convert term to index (e.g., term1 to 0)\n",
    "        score *= dirichlet_smoothed_probability(term_idx, doc_idx, term_document_matrix, mu, term_prob_collection)\n",
    "    return score\n",
    "\n",
    "# Calculate the scores for each document for the given query\n",
    "query = [\"term1\", \"term3\", \"term5\"]\n",
    "scores = [document_score(query, doc_idx, term_document_matrix, mu, term_prob_collection) for doc_idx in range(term_document_matrix.shape[1])]\n",
    "top_scoring_doc = np.argmax(scores) + 1 # adding 1 to match document numbering\n",
    "\n",
    "#print with 3 decimal places (prob_term5_doc1, prob_term4_collection, prob_term2_doc3, lowest_prob_term_doc2, top_scoring_doc)\n",
    "print(f\"Probability of term5 in doc1 is {prob_term5_doc1:.3f}\")\n",
    "print(f\"Probability of term4 in the collection is {prob_term4_collection:.3f}\")\n",
    "print(f\"Probability of term2 in doc3 is {prob_term2_doc3:.3f}\")\n",
    "print(f\"Term with lowest probability in doc2 is term{lowest_prob_term_doc2}\")\n",
    "print(f\"Document with highest score is doc{top_scoring_doc}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def calculate_scores(csv_path):\n",
    "    # Read the CSV file\n",
    "    data = pd.read_csv(csv_path)\n",
    "    y_true = data['actual']  # Replace 'actual' with the column name for actual labels\n",
    "    y_pred = data['predicted']  # Replace 'predicted' with the column name for predicted labels\n",
    "\n",
    "    # Calculate TP, FP, FN, and TN\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "\n",
    "    # Calculate precision, recall, F1, and F2 scores\n",
    "    precision = tp / (tp + fp) if (tp + fp) != 0 else 0\n",
    "    recall = tp / (tp + fn) if (tp + fn) != 0 else 0\n",
    "    f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) != 0 else 0\n",
    "    f2_score = 5 * (precision * recall) / (4 * precision + recall) if (4 * precision + recall) != 0 else 0\n",
    "\n",
    "    return precision, recall, f1_score, f2_score\n",
    "\n",
    "# Usage\n",
    "# precision, recall, f1_score, f2_score = calculate_scores('path_to_csv.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.5, 0.75, 0.6, 0.6818181818181818)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_scores(\"results.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DCG@5 for system A: 3.861\n",
      "DCG@5 for system B: 4.893\n",
      "NDCG@10 for system A: 0.693\n",
      "NDCG@10 for system B: 0.780\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "def dcg_at_value(ranked_documents, ground_truth, value):\n",
    "    dcg = ground_truth.get(ranked_documents[0], 0)\n",
    "    for i, doc_id in enumerate(ranked_documents[1:value], start=2):\n",
    "        relevance_score = ground_truth.get(doc_id, 0)\n",
    "        dcg += relevance_score / math.log2(i)  # +2 because we start counting ranks from 1 and log base 2\n",
    "    return dcg\n",
    "\n",
    "def ndcg_at_k(ranked_documents, ground_truth, k):\n",
    "    dcg_max = dcg_at_value(sorted(ground_truth, key=ground_truth.get, reverse=True), ground_truth, k)\n",
    "    if not dcg_max:\n",
    "        return 0\n",
    "    return dcg_at_value(ranked_documents, ground_truth, k) / dcg_max\n",
    "\n",
    "\n",
    "ground_truth_scores = {1: 3, \n",
    "                       2: 2, \n",
    "                       3: 1, \n",
    "                       7: 3}  # Mapping from the document ID to its score\n",
    "\n",
    "# Rankings from the image\n",
    "system_a_ranking = [10, 7, 9, 8, 2, 1, 3, 4, 5, 6]\n",
    "system_b_ranking = [3, 2, 1, 4, 5, 7, 8, 10, 9, 6]\n",
    "\n",
    "# Calculate DCG@5 for both systems\n",
    "dcg_at_5_system_a = dcg_at_value(system_a_ranking, ground_truth_scores, 5)\n",
    "dcg_at_5_system_b = dcg_at_value(system_b_ranking, ground_truth_scores, 5)\n",
    "\n",
    "print(f\"DCG@5 for system A: {dcg_at_5_system_a:.3f}\")\n",
    "print(f\"DCG@5 for system B: {dcg_at_5_system_b:.3f}\")\n",
    "\n",
    "# calculate ndcg@10 for both systems\n",
    "ndcg_at_10_system_a = ndcg_at_k(system_a_ranking, ground_truth_scores, 10)\n",
    "ndcg_at_10_system_b = ndcg_at_k(system_b_ranking, ground_truth_scores, 10)\n",
    "\n",
    "print(f\"NDCG@10 for system A: {ndcg_at_10_system_a:.3f}\")\n",
    "print(f\"NDCG@10 for system B: {ndcg_at_10_system_b:.3f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2857142857142857"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Function to calculate the probability P(t|c) given a term-document matrix and a class label\n",
    "def calculate_probability(df, term, given_class):\n",
    "    # Count the number of occurrences of the term in documents of the given class\n",
    "    n_t_c = df[df['class'] == given_class][term].sum()\n",
    "    \n",
    "    # Count the total number of terms in documents of the given class\n",
    "    total_terms = df[df['class'] == given_class].drop(columns='class').sum().sum()\n",
    "    \n",
    "    # Count the number of unique classes\n",
    "    num_classes = df['class'].nunique()\n",
    "    \n",
    "    # Calculate the probability using the given formula\n",
    "    probability = (n_t_c + 1) / (total_terms + num_classes)\n",
    "    return probability\n",
    "\n",
    "# Creating a dataframe from the data in the image\n",
    "data = {\n",
    "    't1': [2, 0, 3, 4, 1, 0],\n",
    "    't2': [0, 0, 4, 0, 0, 1],\n",
    "    't3': [1, 0, 0, 3, 0, 1],\n",
    "    't4': [2, 0, 2, 1, 3, 0],\n",
    "    't5': [0, 3, 0, 1, 1, 3],\n",
    "    't6': [2, 2, 0, 1, 2, 4],\n",
    "    't7': [4, 2, 2, 0, 0, 1],\n",
    "    'class': ['C1', 'C3', 'C2', 'C3', 'C2', 'C1']\n",
    "}\n",
    "\n",
    "# Converting dictionary to DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Example calculation for term 't1' and class 'C1'\n",
    "calculate_probability(df, 't4', 'C2')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Fraction(1, 3),\n",
       " Fraction(2, 7),\n",
       " Fraction(1, 8),\n",
       " Fraction(1, 160),\n",
       " 'C2',\n",
       " {'C1': Fraction(1, 144), 'C3': Fraction(1, 120), 'C2': Fraction(4, 441)})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from fractions import Fraction\n",
    "\n",
    "# Define the document-term matrix and the classes\n",
    "data = {\n",
    "    't1': [2, 0, 3, 4, 1, 0],\n",
    "    't2': [0, 0, 4, 0, 0, 1],\n",
    "    't3': [1, 0, 0, 3, 0, 1],\n",
    "    't4': [2, 0, 2, 1, 3, 0],\n",
    "    't5': [0, 3, 0, 1, 1, 3],\n",
    "    't6': [2, 2, 0, 1, 2, 4],\n",
    "    't7': [4, 2, 2, 0, 0, 1],\n",
    "    'class': ['C1', 'C3', 'C2', 'C3', 'C2', 'C1']\n",
    "}\n",
    "# Create a DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Count the number of documents per class\n",
    "class_counts = df['class'].value_counts()\n",
    "\n",
    "# Number of classes\n",
    "num_classes = len(class_counts)\n",
    "\n",
    "# Prior class probability for C2\n",
    "prior_C2 = Fraction(class_counts['C2'], sum(class_counts))\n",
    "\n",
    "# Calculate the term frequency for each term in each class\n",
    "term_frequencies = df.groupby('class').sum()\n",
    "\n",
    "# Total number of terms in class C2\n",
    "total_terms_C2 = term_frequencies.loc['C2'].sum()\n",
    "\n",
    "# Smoothed probability of term \"t4\" belonging to C2 with Laplace smoothing\n",
    "prob_t4_C2 = Fraction(term_frequencies.at['C2', 't4'] + 1, total_terms_C2 + num_classes)\n",
    "\n",
    "# For a new document \"t1\", calculate the probability of belonging to C1\n",
    "# Count of t1 in C1 + 1 / Total count in C1 + Number of terms\n",
    "prob_t1_C1 = Fraction(term_frequencies.at['C1', 't1'] + 1, term_frequencies.loc['C1'].sum() + num_classes)\n",
    "\n",
    "# For a new document \"t1 t4 t5\", calculate the probability of belonging to C3\n",
    "# We'll calculate the individual probabilities and later multiply them\n",
    "prob_t1_C3 = Fraction(term_frequencies.at['C3', 't1'] + 1, term_frequencies.loc['C3'].sum() + num_classes)\n",
    "prob_t4_C3 = Fraction(term_frequencies.at['C3', 't4'] + 1, term_frequencies.loc['C3'].sum() + num_classes)\n",
    "prob_t5_C3 = Fraction(term_frequencies.at['C3', 't5'] + 1, term_frequencies.loc['C3'].sum() + num_classes)\n",
    "prob_t1t4t5_C3 = prob_t1_C3 * prob_t4_C3 * prob_t5_C3\n",
    "\n",
    "# Calculate the class probability of document \"t4 t5\" for each class\n",
    "# Initialize a dictionary to store probabilities for \"t4 t5\" for each class\n",
    "class_probabilities = {}\n",
    "\n",
    "for c in class_counts.index:\n",
    "    prob_t4_c = Fraction(term_frequencies.at[c, 't4'] + 1, term_frequencies.loc[c].sum() + num_classes)\n",
    "    prob_t5_c = Fraction(term_frequencies.at[c, 't5'] + 1, term_frequencies.loc[c].sum() + num_classes)\n",
    "    # Multiply by the prior for the class\n",
    "    class_probabilities[c] = prob_t4_c * prob_t5_c * Fraction(class_counts[c], sum(class_counts))\n",
    "\n",
    "# The classification of \"t4 t5\" should be the class with the highest probability\n",
    "classification_t4_t5 = max(class_probabilities, key=class_probabilities.get)\n",
    "\n",
    "prior_C2, prob_t4_C2, prob_t1_C1, prob_t1t4t5_C3, classification_t4_t5, class_probabilities\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entity retrival"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Porter Stemmer: run\n",
      "Krovetz Stemmer: running\n",
      "Suffix_s Stemmer: running\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "# Placeholder for Krovetz stemmer; this would require a more complex implementation or a specialized library.\n",
    "def krovetz_stemmer(word):\n",
    "    # Implement Krovetz stemming logic here or use a library that provides this stemmer.\n",
    "    return word\n",
    "\n",
    "# Simple Suffix-S Stemmer\n",
    "def suffix_s_stemmer(word):\n",
    "    if word.endswith('s'):\n",
    "        return word[:-1]\n",
    "    return word\n",
    "\n",
    "def apply_stemming(word, stemmer_type):\n",
    "    stemmer = None\n",
    "\n",
    "    if stemmer_type == 'porter':\n",
    "        stemmer = PorterStemmer()\n",
    "        return stemmer.stem(word)\n",
    "    elif stemmer_type == 'krovetz':\n",
    "        return krovetz_stemmer(word)\n",
    "    elif stemmer_type == 'suffix_s':\n",
    "        return suffix_s_stemmer(word)\n",
    "    else:\n",
    "        raise ValueError(\"Invalid stemmer type. Choose 'porter', 'krovetz', or 'suffix_s'.\")\n",
    "\n",
    "# Example usage\n",
    "word = \"running\"\n",
    "for stemmer_type in ['porter', 'krovetz', 'suffix_s']:\n",
    "    stemmed_word = apply_stemming(word, stemmer_type)\n",
    "    print(f\"{stemmer_type.capitalize()} Stemmer: {stemmed_word}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'two faders and two sons went fish'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string = \"Two faders and two sons went fishing\"\n",
    "\n",
    "apply_stemming(string, 'porter')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['activity -> d2:1',\n",
       " 'age -> d3:1',\n",
       " 'benefit -> d3:1',\n",
       " 'fishing -> d1:1 d2:1 d4:1',\n",
       " 'hi -> d1:1',\n",
       " 'i -> d2:1',\n",
       " 'implication -> d2:1',\n",
       " 'important -> d2:1',\n",
       " 'introduction -> d3:1 d4:1',\n",
       " 'lake -> d4:1',\n",
       " 'man -> d1:1',\n",
       " 'old -> d1:1 d3:1',\n",
       " 'protection -> d3:1',\n",
       " 'recreational -> d2:1',\n",
       " 'social -> d2:1 d3:1',\n",
       " 'son -> d1:1',\n",
       " 'trout -> d4:1',\n",
       " 'two -> d1:1',\n",
       " 'went -> d1:1',\n",
       " 'work -> d4:1']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "from collections import defaultdict\n",
    "\n",
    "# Define the documents\n",
    "docs = {\n",
    "    1: \"The old man and his two sons went fishing.\",\n",
    "    2: \"Recreational fishing is an activity with important social implications.\",\n",
    "    3: \"Introduction to social protection benefits for old age.\",\n",
    "    4: \"Introduction to how lake trout fishing works.\"\n",
    "}\n",
    "\n",
    "# Define the stopwords\n",
    "stopwords = set([\n",
    "    \"an\", \"and\", \"are\", \"for\", \"how\", \"in\", \"is\", \"not\", \"or\", \"the\", \"these\", \"this\", \"to\", \"with\"\n",
    "])\n",
    "\n",
    "# Function to preprocess the documents\n",
    "def preprocess(text):\n",
    "    # Tokenize by word\n",
    "    tokens = re.findall(r'\\b\\w+\\b', text.lower())\n",
    "    # Stemming: remove trailing 's' character\n",
    "    tokens = [re.sub(r's$', '', token) for token in tokens]\n",
    "    # Remove stopwords\n",
    "    tokens = [token for token in tokens if token not in stopwords]\n",
    "    return tokens\n",
    "\n",
    "# Create the inverted index\n",
    "inverted_index = defaultdict(lambda: defaultdict(int))\n",
    "\n",
    "for doc_id, text in docs.items():\n",
    "    tokens = preprocess(text)\n",
    "    for token in tokens:\n",
    "        inverted_index[token][doc_id] += 1\n",
    "\n",
    "# Convert the inverted index into a posting list format\n",
    "posting_lists = []\n",
    "for token, doc_freqs in inverted_index.items():\n",
    "    posting_list = f\"{token} ->\" + ''.join([f\" d{doc_id}:{freq}\" for doc_id, freq in doc_freqs.items()])\n",
    "    posting_lists.append(posting_list)\n",
    "\n",
    "posting_lists.sort()\n",
    "posting_lists\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## text similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.375"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#jaccard\n",
    "# Here is the code from the image transcribed into executable Python code.\n",
    "\n",
    "def text_similarity(doc_1: str, doc_2: str) -> float:\n",
    "    set1, set2 = set(doc_1.split()), set(doc_2.split())\n",
    "    return len(set1.intersection(set2)) / len(set1.union(set2))\n",
    "\n",
    "# Example usage of the function with two hypothetical documents\n",
    "doc1 = \"this is some piece of text\"\n",
    "doc2 = \"slightly different piece of text\"\n",
    "\n",
    "# Calculate the similarity between doc1 and doc2\n",
    "similarity_score = text_similarity(doc1, doc2)\n",
    "similarity_score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## bm25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.1087160262417997"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "# Constants from the image\n",
    "N = 1000  # Total number of documents\n",
    "avgdl = 50  # Average document length\n",
    "k1 = 1.25\n",
    "b = 0.8\n",
    "\n",
    "# Term frequency table from the image (here using only the available data)\n",
    "term_frequencies = {\n",
    "    'T1': {'Doc1': 3, 'Doc2': 4, 'Collection': 100},\n",
    "    'T2': {'Doc1': 0, 'Doc2': 3, 'Collection': 50},\n",
    "    'T3': {'Doc1': 2, 'Doc2': 3, 'Collection': 80},\n",
    "    'T4': {'Doc1': 1, 'Doc2': 2, 'Collection': 93},\n",
    "    'T5': {'Doc1': 10, 'Doc2': 1, 'Collection': 100},\n",
    "    'T6': {'Doc1': 5, 'Doc2': 7, 'Collection': 25},\n",
    "}\n",
    "\n",
    "# Document lengths from the image\n",
    "doc_lengths = {\n",
    "    'Doc1': 21,\n",
    "    'Doc2': 20\n",
    "}\n",
    "\n",
    "# Function to calculate BM25 for a single term in a document\n",
    "def bm25(doc_id, term_id):\n",
    "    # Extract the necessary information for the calculations\n",
    "    ft_d = term_frequencies[term_id].get(doc_id, 0)  # frequency of term in document\n",
    "    nt = term_frequencies[term_id]['Collection']  # number of documents containing the term\n",
    "    doc_length = doc_lengths[doc_id]  # length of the document\n",
    "\n",
    "    # Calculate idft using base-10 logarithm\n",
    "    idft = math.log10(N / nt)\n",
    "\n",
    "    # Calculate the BM25 score\n",
    "    numerator = ft_d * (1 + k1)\n",
    "    denominator = ft_d + k1 * (1 - b + b * (doc_length / avgdl))\n",
    "    score = idft * (numerator / denominator)\n",
    "    \n",
    "    return score\n",
    "\n",
    "# Example usage of the function\n",
    "# Get the BM25 score for Term 1 in Document 1\n",
    "bm25_score = bm25('Doc1', 'T5')\n",
    "bm25_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1087160262417997\n"
     ]
    }
   ],
   "source": [
    "multi = bm25('Doc1', 'T2')+bm25('Doc1', 'T2')+bm25('Doc1', 'T5')\n",
    "print(multi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.175665114722323\n"
     ]
    }
   ],
   "source": [
    "multi = bm25('Doc2', 'T2')+bm25('Doc2', 'T2')+bm25('Doc2', 'T5')\n",
    "print(multi)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "text",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
