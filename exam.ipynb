{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## sec 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean average precision for Algorithm A: 0.394\n",
      "mean average precision for Algorithm B: 0.353\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.4, 0.625, 0.5, 0.35, 'Algorithm A')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extracting data from the OCR output\n",
    "rankings = {\n",
    "    \"Algorithm A\": {\n",
    "        \"Query 1\": [1, 2, 6, 5, 9, 10, 7, 4, 8, 3],\n",
    "        \"Query 2\": [1, 2, 4, 5, 7, 10, 8, 3, 9, 6]\n",
    "    },\n",
    "    \"Algorithm B\": {\n",
    "        \"Query 1\": [10, 9, 8, 7, 5, 4, 6, 2, 1, 3],\n",
    "        \"Query 2\": [1, 3, 2, 4, 5, 6, 8, 7, 10, 9]\n",
    "    },\n",
    "    \"Ground truth\": {\n",
    "        \"Query 1\": [1, 4, 5],\n",
    "        \"Query 2\": [3, 6]\n",
    "    }\n",
    "}\n",
    "\n",
    "# Helper functions to calculate metrics\n",
    "def calculate_precision_at_k(ranking, ground_truth, k):\n",
    "    relevant_docs = set(ground_truth)\n",
    "    retrieved_docs_at_k = ranking[:k]\n",
    "    relevant_at_k = [doc for doc in retrieved_docs_at_k if doc in relevant_docs]\n",
    "    precision_at_k = len(relevant_at_k) / k\n",
    "    return precision_at_k\n",
    "\n",
    "def calculate_average_precision(ranking, ground_truth):\n",
    "    relevant_docs = set(ground_truth)\n",
    "    relevant_found = 0\n",
    "    precision_sum = 0\n",
    "\n",
    "    for rank, doc in enumerate(ranking, start=1):\n",
    "        if doc in relevant_docs:\n",
    "            relevant_found += 1\n",
    "            precision_sum += relevant_found / rank\n",
    "\n",
    "    average_precision = precision_sum / len(relevant_docs)\n",
    "    return average_precision\n",
    "\n",
    "def calculate_reciprocal_rank(ranking, ground_truth):\n",
    "    relevant_docs = set(ground_truth)\n",
    "    for rank, doc in enumerate(ranking, start=1):\n",
    "        if doc in relevant_docs:\n",
    "            return 1 / rank\n",
    "    return 0  # If no relevant document is found\n",
    "\n",
    "def calculate_mean_reciprocal_rank(rankings, ground_truths):\n",
    "    rr_sum = 0\n",
    "    for query in ground_truths:\n",
    "        rr_sum += calculate_reciprocal_rank(rankings[query], ground_truths[query])\n",
    "    mean_reciprocal_rank = rr_sum / len(ground_truths)\n",
    "    return mean_reciprocal_rank\n",
    "\n",
    "# Calculating the required metrics\n",
    "p_at_5_query_1_A = calculate_precision_at_k(rankings[\"Algorithm A\"][\"Query 1\"], rankings[\"Ground truth\"][\"Query 1\"], 5)\n",
    "avg_precision_query_1_A = calculate_average_precision(rankings[\"Algorithm A\"][\"Query 1\"], rankings[\"Ground truth\"][\"Query 1\"])\n",
    "reciprocal_rank_query_2_B = calculate_reciprocal_rank(rankings[\"Algorithm B\"][\"Query 2\"], rankings[\"Ground truth\"][\"Query 2\"])\n",
    "mean_reciprocal_rank_B = calculate_mean_reciprocal_rank(rankings[\"Algorithm B\"], rankings[\"Ground truth\"])\n",
    "\n",
    "# Comparing Average Precision between algorithms (for Query 1 only, as no data for Query 2)\n",
    "avg_precision_query_1_B = calculate_average_precision(rankings[\"Algorithm B\"][\"Query 1\"], rankings[\"Ground truth\"][\"Query 1\"])\n",
    "avg_precision_query_2_A = calculate_average_precision(rankings[\"Algorithm A\"][\"Query 2\"], rankings[\"Ground truth\"][\"Query 2\"])\n",
    "avg_precision_query_2_B = calculate_average_precision(rankings[\"Algorithm B\"][\"Query 2\"], rankings[\"Ground truth\"][\"Query 2\"])\n",
    "mean_avg_precision_A = (avg_precision_query_1_A + avg_precision_query_2_A) / 2\n",
    "mean_avg_precision_B = (avg_precision_query_1_B + avg_precision_query_2_B) / 2\n",
    "higher_map = \"Algorithm A\" if mean_avg_precision_A > mean_avg_precision_B else \"Algorithm B\"\n",
    "\n",
    "print(\"mean average precision for Algorithm A: {:.3f}\".format(mean_avg_precision_A))\n",
    "print(\"mean average precision for Algorithm B: {:.3f}\".format(mean_avg_precision_B))\n",
    "\n",
    "p_at_5_query_1_A, avg_precision_query_1_A, reciprocal_rank_query_2_B, mean_reciprocal_rank_B, higher_map\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.26185950714291506"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "# Given values\n",
    "Le3 = 2  # Number of incoming links to entity e3\n",
    "Le6 = 4  # Number of incoming links to entity e6\n",
    "Le3_inter_e6 = 1  # Number of common incoming links\n",
    "E = 6  # Total number of entities\n",
    "\n",
    "# WLM formula calculation using base 2 logs\n",
    "WLM_e3_e6 = 1 - (math.log(max(Le3, Le6), 2) - math.log(Le3_inter_e6, 2)) / (math.log(E, 2) - math.log(min(Le3, Le6), 2))\n",
    "\n",
    "WLM_e3_e6\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## section2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'entity1': 'that mentions entity-one alone ABC and entity-one together',\n",
       " 'entity2': 'document with ABC and entity-one yyy zzz ABC aaa bbb',\n",
       " 'entity3': 'ccc ddd ZZZ eee fff'}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "from typing import List, Dict\n",
    "\n",
    "def create_entity_repr(docs: List[str], window_size: int) -> Dict[str, str]:\n",
    "    entity_repr = {}\n",
    "\n",
    "    for doc in docs:\n",
    "        # Find all entity annotations in the document\n",
    "        entities = re.findall(r'\\[\\[(.*?)\\|(.*?)\\]\\]', doc)\n",
    "        \n",
    "        # Replace the entity annotations with a placeholder to prevent overlapping\n",
    "        for entity in entities:\n",
    "            doc = doc.replace(f'[[{entity[0]}|{entity[1]}]]', f' {entity[1]} ', 1)\n",
    "        \n",
    "        # Split the document into terms after replacing entities\n",
    "        terms = doc.split()\n",
    "        \n",
    "        # Iterate over the entities and create the representation\n",
    "        for entity in entities:\n",
    "            entity_id, entity_text = entity\n",
    "            entity_repr[entity_id] = entity_repr.get(entity_id, '')\n",
    "\n",
    "            # Find all occurrences of the entity text in terms\n",
    "            for i, term in enumerate(terms):\n",
    "                if term == entity_text:\n",
    "                    # Get the window of terms around the entity mention\n",
    "                    window_terms = terms[max(0, i - window_size):i] + \\\n",
    "                                   terms[i + 1:min(len(terms), i + window_size + 1)]\n",
    "                    # Add the entity mention itself\n",
    "                    window_terms.insert(window_size, entity_text)\n",
    "                    # Add to the entity representation\n",
    "                    entity_repr[entity_id] += ' ' + ' '.join(window_terms)\n",
    "                    entity_repr[entity_id] = entity_repr[entity_id].strip()\n",
    "\n",
    "    return entity_repr\n",
    "\n",
    "# Test the function with the provided example input\n",
    "docs_example = [\n",
    "    \"first document that mentions [[entity1|entity-one]] alone\",\n",
    "    \"2nd document with [[entity2|ABC]] and [[entity1|entity-one]] together\",\n",
    "    \"xxx yyy zzz [[entity2|ABC]] aaa bbb ccc ddd [[entity3|ZZZ]] eee fff\",\n",
    "]\n",
    "\n",
    "# Expected output with window_size=2\n",
    "# {\n",
    "#     \"entity1\": \"that mentions entity one-alone ABC and entity-one together\",\n",
    "#     \"entity2\": \"document with ABC and entity-one yyy zzz ABC aaa bbb\",\n",
    "#     \"entity3\": \"ccc ddd ZZZ eee fff\",\n",
    "# }\n",
    "\n",
    "# We'll test the function again with the same example input.\n",
    "create_entity_repr(docs_example, window_size=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('The', 'quick'), ('The', 'brown'), ('quick', 'The'), ('quick', 'brown'), ('quick', 'fox'), ('brown', 'The'), ('brown', 'quick'), ('brown', 'fox'), ('brown', 'jumps'), ('fox', 'quick'), ('fox', 'brown'), ('fox', 'jumps'), ('fox', 'over'), ('jumps', 'brown'), ('jumps', 'fox'), ('jumps', 'over'), ('jumps', 'the'), ('over', 'fox'), ('over', 'jumps'), ('over', 'the'), ('over', 'lazy'), ('the', 'jumps'), ('the', 'over'), ('the', 'lazy'), ('the', 'dog'), ('lazy', 'over'), ('lazy', 'the'), ('lazy', 'dog'), ('dog', 'the'), ('dog', 'lazy')]\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "def generate_skipgram_examples(sentence, window_size=2):\n",
    "    \"\"\"\n",
    "    Generate positive training examples for Skip-gram model from a given sentence.\n",
    "\n",
    "    :param sentence: A string representing the input sentence.\n",
    "    :param window_size: The size of the context window.\n",
    "    :return: A list of tuples, each containing the target word and a context word.\n",
    "    \"\"\"\n",
    "    # Tokenize the sentence\n",
    "    words = sentence.split()\n",
    "\n",
    "    # Generate positive context pairs\n",
    "    positive_pairs = []\n",
    "    for index, target in enumerate(words):\n",
    "        # Define the context window range\n",
    "        start = max(0, index - window_size)\n",
    "        end = min(len(words), index + window_size + 1)\n",
    "\n",
    "        # Generate pairs (target, context)\n",
    "        for context_index in range(start, end):\n",
    "            if context_index != index:\n",
    "                positive_pairs.append((target, words[context_index]))\n",
    "\n",
    "    return positive_pairs\n",
    "\n",
    "# Example usage\n",
    "sentence = \"The quick brown fox jumps over the lazy dog\"\n",
    "examples = generate_skipgram_examples(sentence)\n",
    "print(examples)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## sec 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def sdm_with_dirichlet_smoothing(query, documents, mu=6, window_size=4, weight_single=0.85, weight_ordered=0.1, weight_unordered=0.05):\n",
    "\n",
    "    query_terms = query.split()\n",
    "    query_ordered_pairs = [(query_terms[i], query_terms[i + 1]) for i in range(len(query_terms) - 1)]\n",
    "    query_unordered_pairs = [(query_terms[i], query_terms[j]) for i in range(len(query_terms)) for j in range(i + 1, min(i + window_size, len(query_terms)))]\n",
    "\n",
    "    # Initialize scores for each document\n",
    "    scores = {doc: 0 for doc in documents}\n",
    "\n",
    "    # Calculate the length of the corpus\n",
    "    corpus_length = sum(len(doc.split()) for doc in documents)\n",
    "\n",
    "    # Calculate term and pair frequencies in the corpus\n",
    "    corpus_term_freq = {}\n",
    "    corpus_ordered_pair_freq = {}\n",
    "    corpus_unordered_pair_freq = {}\n",
    "    for doc in documents:\n",
    "        words = doc.split()\n",
    "        for term in query_terms:\n",
    "            corpus_term_freq[term] = corpus_term_freq.get(term, 0) + words.count(term)\n",
    "        for pair in query_ordered_pairs:\n",
    "            corpus_ordered_pair_freq[pair] = corpus_ordered_pair_freq.get(pair, 0) + sum(1 for i in range(len(words) - 1) if words[i] == pair[0] and words[i + 1] == pair[1])\n",
    "        for i in range(len(words)):\n",
    "            for j in range(i + 1, min(i + window_size, len(words))):\n",
    "                pair = (words[i], words[j])\n",
    "                if pair in query_unordered_pairs or pair[::-1] in query_unordered_pairs:\n",
    "                    corpus_unordered_pair_freq[pair] = corpus_unordered_pair_freq.get(pair, 0) + 1\n",
    "\n",
    "    # Scoring documents\n",
    "    for doc in documents:\n",
    "        doc_length = len(doc.split())\n",
    "        for term in query_terms:\n",
    "            term_freq_in_doc = doc.split().count(term)\n",
    "            term_prob = (term_freq_in_doc + mu * (corpus_term_freq[term] / corpus_length)) / (doc_length + mu)\n",
    "            scores[doc] += weight_single * math.log(term_prob)\n",
    "\n",
    "        for pair in query_ordered_pairs:\n",
    "            pair_freq_in_doc = sum(1 for i in range(len(doc.split()) - 1) if doc.split()[i] == pair[0] and doc.split()[i + 1] == pair[1])\n",
    "            pair_prob = (pair_freq_in_doc + mu * (corpus_ordered_pair_freq.get(pair, 0) / corpus_length)) / (doc_length + mu)\n",
    "            scores[doc] += weight_ordered * math.log(pair_prob)\n",
    "\n",
    "        for pair in query_unordered_pairs:\n",
    "            pair_freq_in_doc = 0\n",
    "            words = doc.split()\n",
    "            for i in range(len(words)):\n",
    "                for j in range(i + 1, min(i + window_size, len(words))):\n",
    "                    if (words[i], words[j]) == pair or (words[j], words[i]) == pair:\n",
    "                        pair_freq_in_doc += 1\n",
    "            pair_prob = (pair_freq_in_doc + mu * (corpus_unordered_pair_freq.get(pair, 0) / corpus_length)) / (doc_length + mu)\n",
    "            scores[doc] += weight_unordered * math.log(pair_prob)\n",
    "\n",
    "    return scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'t1 t2 t3 t4 t5 t6': -3.5049499402474984,\n",
       " 't3 t4 t3 t1 t8 t2 t2 t7': -3.264782173133931,\n",
       " 't2 t9 t4 t1 t8 t2 t3 t1 t4': -3.579170015890382}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs = [\n",
    "    \"t1 t2 t3 t4 t5 t6\",\n",
    "    \"t3 t4 t3 t1 t8 t2 t2 t7\",\n",
    "    \"t2 t9 t4 t1 t8 t2 t3 t1 t4\"\n",
    "]\n",
    "query = \"t4 t3\"\n",
    "\n",
    "sdm_with_dirichlet_smoothing(query, docs)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jaccard Similarity: 0.5\n",
      "Cosine Similarity: 0.6666666666666667\n"
     ]
    }
   ],
   "source": [
    "a = [1, 0, 0, 1, 1, 0, 1, 1, 0, 1]\n",
    "b = [1,1,0,1,0,0,1,0,1,1]\n",
    "import numpy as np\n",
    "\n",
    "def calculate_similarity(vec1, vec2):\n",
    "    # Ensure the vectors are NumPy arrays\n",
    "    vec1, vec2 = np.array(vec1), np.array(vec2)\n",
    "\n",
    "    # Jaccard Similarity\n",
    "    intersection = np.sum(np.logical_and(vec1, vec2))\n",
    "    union = np.sum(np.logical_or(vec1, vec2))\n",
    "    jaccard_similarity = intersection / union if union != 0 else 0\n",
    "\n",
    "    # Cosine Similarity\n",
    "    dot_product = np.dot(vec1, vec2)\n",
    "    magnitude_vec1 = np.linalg.norm(vec1)\n",
    "    magnitude_vec2 = np.linalg.norm(vec2)\n",
    "    cosine_similarity = dot_product / (magnitude_vec1 * magnitude_vec2) if magnitude_vec1 != 0 and magnitude_vec2 != 0 else 0\n",
    "\n",
    "    return jaccard_similarity, cosine_similarity\n",
    "\n",
    "# Example usage\n",
    "\n",
    "jaccard_sim, cosine_sim = calculate_similarity(b, a)\n",
    "print(\"Jaccard Similarity:\", jaccard_sim)\n",
    "print(\"Cosine Similarity:\", cosine_sim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_query_rewrites(topics: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Create query rewrites.\n",
    "    \n",
    "    Args:\n",
    "        topics: A dataframe containing the queries for each topic.\n",
    "\n",
    "    Returns:\n",
    "        Modified dataframe with the query rewrites.\n",
    "    \"\"\"\n",
    "    rewrites = pd.DataFrame()\n",
    "    \n",
    "    # Iterate through each group of topics\n",
    "    for _, topic in topics.groupby(\"topic_number\"):\n",
    "        topic.reset_index(inplace=True, drop=True)\n",
    "        \n",
    "        # Keep track of the previous query\n",
    "        previous_query = None\n",
    "        \n",
    "        for i, row in topic.iterrows():\n",
    "            # Skip the first query of each topic since there's no previous query to rewrite from\n",
    "            if i == 0:\n",
    "                previous_query = row[\"query\"]\n",
    "                topic.at[i, \"rewrite\"] = row[\"query\"]\n",
    "                continue\n",
    "            \n",
    "            # Encode the previous and current query and concatenate them with the separator\n",
    "            input_text = f\"{previous_query} {SEPARATOR} {row['query']}\"\n",
    "            input_ids = tokenizer.encode(input_text, return_tensors='pt')\n",
    "            \n",
    "            # Generate the rewritten query\n",
    "            outputs = model.generate(input_ids, max_length=512, early_stopping=True)\n",
    "            \n",
    "            # Decode the generated tokens to get the rewritten string\n",
    "            rewrite = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "            \n",
    "            # Assign the rewrite to the current row\n",
    "            topic.at[i, \"rewrite\"] = rewrite\n",
    "            \n",
    "            # Update the previous query\n",
    "            previous_query = rewrite\n",
    "\n",
    "        # Append the rewrites of the current topic to the main rewrites dataframe\n",
    "        rewrites = pd.concat([rewrites, topic])\n",
    "    \n",
    "    return rewrites\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "text",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
