{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Exploring Word2Vec with Gensim"
      ],
      "metadata": {
        "id": "8erErqJlrN8V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Overview"
      ],
      "metadata": {
        "id": "P9FEOaWsrN8X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Word2Vec is an approach to learning *word embeddings*, vector representations of words that capture semantic and syntactic relationships between words based on their co-occurrences in natural language text.\n",
        "\n",
        "This unsupervised learning approach also reduces the dimensionality of the vectors representing words, which can be helpful for memory and to manage the *curse of dimensionality*, whereby high-dimensional vector spaces lead to a relative data sparsity, e.g., for machine learning."
      ],
      "metadata": {
        "id": "km-s_ja2rN8X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this exercise you will look at the capabilities of Word2Vec as implemented in the module Gensim."
      ],
      "metadata": {
        "id": "uDcuhEiBrN8X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Requirements"
      ],
      "metadata": {
        "id": "pW5GlR2PrN8Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Uncomment the lines below, run the installations once as needed, then comment the code out again."
      ],
      "metadata": {
        "id": "JHwayhworN8Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install ipytest"
      ],
      "metadata": {
        "id": "iE3Ev6HLr9kL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "94ee24a7-a345-4125-a6d0-dd4e4d2fcc6f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ipytest\n",
            "  Downloading ipytest-0.13.3-py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: ipython in /usr/local/lib/python3.10/dist-packages (from ipytest) (7.34.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from ipytest) (23.2)\n",
            "Requirement already satisfied: pytest>=5.4 in /usr/local/lib/python3.10/dist-packages (from ipytest) (7.4.2)\n",
            "Requirement already satisfied: iniconfig in /usr/local/lib/python3.10/dist-packages (from pytest>=5.4->ipytest) (2.0.0)\n",
            "Requirement already satisfied: pluggy<2.0,>=0.12 in /usr/local/lib/python3.10/dist-packages (from pytest>=5.4->ipytest) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup>=1.0.0rc8 in /usr/local/lib/python3.10/dist-packages (from pytest>=5.4->ipytest) (1.1.3)\n",
            "Requirement already satisfied: tomli>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from pytest>=5.4->ipytest) (2.0.1)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.10/dist-packages (from ipython->ipytest) (67.7.2)\n",
            "Collecting jedi>=0.16 (from ipython->ipytest)\n",
            "  Downloading jedi-0.19.1-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from ipython->ipytest) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.10/dist-packages (from ipython->ipytest) (0.7.5)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.10/dist-packages (from ipython->ipytest) (5.7.1)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from ipython->ipytest) (3.0.39)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.10/dist-packages (from ipython->ipytest) (2.16.1)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.10/dist-packages (from ipython->ipytest) (0.2.0)\n",
            "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.10/dist-packages (from ipython->ipytest) (0.1.6)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.10/dist-packages (from ipython->ipytest) (4.8.0)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.3 in /usr/local/lib/python3.10/dist-packages (from jedi>=0.16->ipython->ipytest) (0.8.3)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.10/dist-packages (from pexpect>4.3->ipython->ipytest) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython->ipytest) (0.2.8)\n",
            "Installing collected packages: jedi, ipytest\n",
            "Successfully installed ipytest-0.13.3 jedi-0.19.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# !pip install --upgrade pip\n",
        "# !pip install --upgrade Cython\n",
        "# !pip install --upgrade gensim"
      ],
      "outputs": [],
      "metadata": {
        "id": "f7Pgt9EorN8Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import all necessary libraries."
      ],
      "metadata": {
        "id": "-x1hXqJArN8Z"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# Import modules and set up logging.\n",
        "from typing import List, Generator\n",
        "import gensim.downloader as api\n",
        "from gensim.models import Word2Vec\n",
        "import logging\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
        "\n",
        "import ipytest\n",
        "import pytest\n",
        "\n",
        "ipytest.autoconfig()"
      ],
      "outputs": [],
      "metadata": {
        "id": "2AnrOe_QrN8Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Download data"
      ],
      "metadata": {
        "id": "P60Uvf8yrN8Z"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# Load the Text8 corpus.\n",
        "print(api.info('text8'))\n",
        "text8_corpus = api.load('text8')"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'num_records': 1701, 'record_format': 'list of str (tokens)', 'file_size': 33182058, 'reader_code': 'https://github.com/RaRe-Technologies/gensim-data/releases/download/text8/__init__.py', 'license': 'not found', 'description': 'First 100,000,000 bytes of plain text from Wikipedia. Used for testing purposes; see wiki-english-* for proper full Wikipedia datasets.', 'checksum': '68799af40b6bda07dfa47a32612e5364', 'file_name': 'text8.gz', 'read_more': ['http://mattmahoney.net/dc/textdata.html'], 'parts': 1}\n",
            "[==================================================] 100.0% 31.6/31.6MB downloaded\n"
          ]
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N7i26KlerN8a",
        "outputId": "57296142-295c-45e0-86f7-8ec8c2656d44"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train a model"
      ],
      "metadata": {
        "id": "WHWVODH5rN8a"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# Train a Word2Vec model on the Text8 corpus with default hyperparameters.\n",
        "model = Word2Vec(text8_corpus)\n",
        "\n",
        "# Perform a sanity check on the trained model.\n",
        "print(model.wv.similarity('tree', 'leaf'))"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.6665878\n"
          ]
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kWbcFsrUrN8a",
        "outputId": "a2b71d3e-7bae-457a-ad42-57d23aa3172f"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# Reduce logging level.\n",
        "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.WARNING)"
      ],
      "outputs": [],
      "metadata": {
        "id": "5RS3ddp3rN8a"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "print(model.wv.most_similar('tree'))\n",
        "print(model.wv.most_similar('leaf'))"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('trees', 0.7079861760139465), ('leaf', 0.6665878295898438), ('bark', 0.6538001894950867), ('vine', 0.6142206788063049), ('fruit', 0.6016198992729187), ('bird', 0.6014313101768494), ('skeleton', 0.574469804763794), ('cave', 0.5741851925849915), ('avl', 0.5740269422531128), ('nest', 0.5717236399650574)]\n",
            "[('bark', 0.7800428867340088), ('coloured', 0.7542052865028381), ('jelly', 0.7346197366714478), ('colored', 0.7331221699714661), ('flower', 0.7298780679702759), ('fried', 0.7292338013648987), ('pollen', 0.7290586829185486), ('abalone', 0.7280126810073853), ('sap', 0.7245625853538513), ('sperm', 0.724285900592804)]\n"
          ]
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rHDVIKlFrN8a",
        "outputId": "36068370-a9c5-4f57-f3f0-6ead255fd593"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Relationships\n",
        "\n",
        "Investigate the relationships between words in terms of trained representations."
      ],
      "metadata": {
        "id": "4JfXVHjPrN8a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluate  analogies\n",
        "With the model you have trained, evaluate the analogy\n",
        "`king-man+woman =~ queen`"
      ],
      "metadata": {
        "id": "fvVb4RZ6rN8a"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "print(model.wv.most_similar(positive=['king', 'woman'], negative=['man'], topn=5))"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('queen', 0.6895353198051453), ('throne', 0.6135202646255493), ('prince', 0.6088247895240784), ('princess', 0.6042246222496033), ('empress', 0.6005150079727173)]\n"
          ]
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZMq0rNp6rN8b",
        "outputId": "ac97c7ee-8878-462c-87ed-fa6ae7759228"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluate the analogy `ship-boat+rocket =~ spacecraft`. How similar are the left-hand side of the analogy to the right-hand side? Implement a function that can find the answer for analogies in general. We assume the right-hand side of the analogy will always be a single, positive term."
      ],
      "metadata": {
        "id": "jNPZWXZArN8b"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "def eval_analogy(model: Word2Vec, lhs_pos: List[str], lhs_neg: List[str], rhs: str)->float:\n",
        "    \"\"\"Returns the similarity between the left-hand and right-hand sides of an anaology.\n",
        "\n",
        "        Arguments:\n",
        "            model: Trained Gensim word2vec model to use.\n",
        "            lhs_pos: List of terms that are positive on the left-hand side in the analogy.\n",
        "            lhs_neg: List of terms that are negative on the left-hand side in the analogy.\n",
        "            rhs: A single positive term on the right-hand side in the analogy.\n",
        "\n",
        "        Returns:\n",
        "            Float of the similarity if right-hand side term is found in the top 500 most similar terms.\n",
        "            Otherwise, return None.\"\"\"\n",
        "    # How similar are the left-hand side of the analogy to the right-hand side?\n",
        "    # Implement a function that can find the answer for analogies in general.\n",
        "    similarities_list = model.most_similar(positive=lhs_pos, negative=lhs_neg, topn=500)\n",
        "    similarities_dict = {}\n",
        "    for term, sim in similarities_list:\n",
        "        similarities_dict[term] = sim\n",
        "    if rhs in similarities_dict:\n",
        "        return similarities_dict[rhs]\n",
        "    else:\n",
        "        print(\"Right-hand side term not found in top 500 most similar terms to the left-hand side analogy.\")\n",
        "        None"
      ],
      "outputs": [],
      "metadata": {
        "id": "4Zv5s2AjrN8b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test:"
      ],
      "metadata": {
        "id": "y47NEZeJrN8b"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "%%run_pytest[clean]\n",
        "\n",
        "def test_eval_analogy():\n",
        "    assert eval_analogy(model.wv, ['ship', 'rocket'], ['boat'], 'spacecraft') == pytest.approx(0.7, abs=1e-1)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[32m.\u001b[0m\u001b[32m                                                                                            [100%]\u001b[0m\n",
            "\u001b[32m\u001b[32m\u001b[1m1 passed\u001b[0m\u001b[32m in 0.02s\u001b[0m\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "%%run_pytest[clean] and %%run_pytest are deprecated in favor of %%ipytest. %%ipytest will clean tests, evaluate the cell and then run pytest. To disable cleaning, configure ipytest with ipytest.config(clean=False).\n",
            "ipytest.clean_tests is deprecated in favor of ipytest.clean\n"
          ]
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HxbmLvSjrN8c",
        "outputId": "da1f3b5c-e6b7-400d-c4aa-c0dc7bf82aee"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load a pre-trained model"
      ],
      "metadata": {
        "id": "y3T6E2a8rN8c"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "import gensim.downloader as api\n",
        "model_loaded = api.load('word2vec-google-news-300')"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[==================================================] 100.0% 1662.8/1662.8MB downloaded\n"
          ]
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4pknof_arN8c",
        "outputId": "ce97eafd-bc92-45de-d160-52ab7e4ff790"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "loaded_analogy_eval = -1\n",
        "# Evaluate the analogy 'king'-'man'+'woman' compared to 'queen' using the loaded model\n",
        "# and assign the value to the variable `loaded_analogy_eval`.\n",
        "loaded_analogy_eval = eval_analogy(model_loaded, ['king', 'woman'], ['man'], 'queen')"
      ],
      "outputs": [],
      "metadata": {
        "id": "zQFYufuxrN8c"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "%%run_pytest[clean]\n",
        "\n",
        "def test_loaded_analogy_eval():\n",
        "    assert loaded_analogy_eval != -1\n",
        "    assert loaded_analogy_eval == pytest.approx(0.7, abs=1e-1)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "%%run_pytest[clean] and %%run_pytest are deprecated in favor of %%ipytest. %%ipytest will clean tests, evaluate the cell and then run pytest. To disable cleaning, configure ipytest with ipytest.config(clean=False).\n",
            "ipytest.clean_tests is deprecated in favor of ipytest.clean\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[32m.\u001b[0m\u001b[32m                                                                                            [100%]\u001b[0m\n",
            "\u001b[32m\u001b[32m\u001b[1m1 passed\u001b[0m\u001b[32m in 0.03s\u001b[0m\u001b[0m\n"
          ]
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E1bW9LVSrN8c",
        "outputId": "d00230c7-4bb3-4d39-9e5c-853b7d65575a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train Word2Vec on different corpora"
      ],
      "metadata": {
        "id": "SrPRdsZdrN8c"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# Download the rap lyrics of Kanye West.\n",
        "! wget https://raw.githubusercontent.com/gsurma/text_predictor/master/data/kanye/input.txt\n",
        "! mv input.txt kanye.txt\n",
        "\n",
        "# Download the complete works of William Shakespeare.\n",
        "! wget https://raw.githubusercontent.com/gsurma/text_predictor/master/data/shakespeare/input.txt\n",
        "! mv input.txt shakespeare.txt"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-10-13 15:28:09--  https://raw.githubusercontent.com/gsurma/text_predictor/master/data/kanye/input.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 330453 (323K) [text/plain]\n",
            "Saving to: ‘input.txt’\n",
            "\n",
            "input.txt           100%[===================>] 322.71K  --.-KB/s    in 0.03s   \n",
            "\n",
            "2023-10-13 15:28:09 (10.1 MB/s) - ‘input.txt’ saved [330453/330453]\n",
            "\n",
            "--2023-10-13 15:28:09--  https://raw.githubusercontent.com/gsurma/text_predictor/master/data/shakespeare/input.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 4573338 (4.4M) [text/plain]\n",
            "Saving to: ‘input.txt’\n",
            "\n",
            "input.txt           100%[===================>]   4.36M  --.-KB/s    in 0.06s   \n",
            "\n",
            "2023-10-13 15:28:10 (70.0 MB/s) - ‘input.txt’ saved [4573338/4573338]\n",
            "\n"
          ]
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hC7wUGL6rN8c",
        "outputId": "fda898b7-e315-4c07-8a92-f358fb4ccbe7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "from gensim.test.utils import datapath\n",
        "from gensim import utils\n",
        "\n",
        "class MyCorpus:\n",
        "    \"\"\"An interator that yields sentences (lists of str).\"\"\"\n",
        "    def __init__(self, data: str) -> None:\n",
        "        self.data = data\n",
        "\n",
        "    def __iter__(self) -> Generator[List[str], None, None]:\n",
        "        corpus_path = datapath(self.data)\n",
        "        for line in open(corpus_path):\n",
        "            # assume there's one document per line, tokens separated by whitespace\n",
        "            yield utils.simple_preprocess(line)"
      ],
      "outputs": [],
      "metadata": {
        "id": "_hUkv9ZirN8d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Separately train two new models using the two different datasets, and compare how these datasets affect relationships between"
      ],
      "metadata": {
        "id": "sOK7dp0vrN8d"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "kanye_data = MyCorpus(os.getcwd()+'/kanye.txt')\n",
        "shakespeare_data = MyCorpus(os.getcwd()+'/shakespeare.txt')"
      ],
      "outputs": [],
      "metadata": {
        "id": "syhlDRCXrN8d"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "kanye_model = None\n",
        "# Train a Word2Vec model on the Kanye corpus, and name it `kanye_model`.\n",
        "kanye_model = Word2Vec(sentences=kanye_data)"
      ],
      "outputs": [],
      "metadata": {
        "id": "SSZUSqyHrN8d"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "shakespeare_model = None\n",
        "# Train a Word2Vec model on the Shakespeare corpus, and name it `shakespeare_model`.\n",
        "shakespeare_model = Word2Vec(sentences=shakespeare_data)"
      ],
      "outputs": [],
      "metadata": {
        "id": "xN4oiKairN8d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For each of the models, we can easily find words where the two models learn very different similarities."
      ],
      "metadata": {
        "id": "kvvBEVm1rN8d"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# For example, compare:\n",
        "print(kanye_model.wv.most_similar(positive=['king'], topn=5))\n",
        "print(shakespeare_model.wv.most_similar(positive=['king'], topn=5))"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('our', 0.9988145232200623), ('big', 0.998805582523346), ('always', 0.9987574219703674), ('as', 0.9987520575523376), ('or', 0.9987413883209229)]\n",
            "[('prince', 0.8835847973823547), ('bolingbroke', 0.7122901678085327), ('duke', 0.6925632953643799), ('crown', 0.6918205618858337), ('fifth', 0.6868830323219299)]\n"
          ]
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1yaa2hWkrN8d",
        "outputId": "d2a05a58-ead7-4e2d-872f-3cce7e1d9226"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For more information about Gensim, see https://radimrehurek.com/gensim."
      ],
      "metadata": {
        "id": "1C8JgtObrN8e"
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3.9.7 64-bit ('dat640': conda)"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "interpreter": {
      "hash": "4f36a626e24aa200704e7a1da8e159d79437a6ae015274136a84a715398481c5"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}