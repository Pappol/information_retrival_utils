{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HWQgIBxfXyzM"
      },
      "source": [
        "# Bigram matches in Elasticsearch\n",
        "\n",
        "This exercise is about getting ordered and unordered bigram matches using Elasticsearch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fl8ghWrbdrTJ",
        "outputId": "6ef2d47b-13ec-4671-9cb9-173b4bd617a7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "elasticsearch-oss-7.9.2-linux-x86_64.tar.gz: OK\n"
          ]
        }
      ],
      "source": [
        "%%bash\n",
        "\n",
        "wget -q https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-oss-7.9.2-linux-x86_64.tar.gz\n",
        "wget -q https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-oss-7.9.2-linux-x86_64.tar.gz.sha512\n",
        "tar -xzf elasticsearch-oss-7.9.2-linux-x86_64.tar.gz\n",
        "sudo chown -R daemon:daemon elasticsearch-7.9.2/\n",
        "shasum -a 512 -c elasticsearch-oss-7.9.2-linux-x86_64.tar.gz.sha512"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1eP24fq7dwpA"
      },
      "outputs": [],
      "source": [
        "%%bash --bg\n",
        "\n",
        "sudo -H -u daemon elasticsearch-7.9.2/bin/elasticsearch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "twQRaoCedyYW"
      },
      "outputs": [],
      "source": [
        "# Sleep for few seconds to let the instance start.\n",
        "import time\n",
        "time.sleep(20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dlUnRTAFd4uX",
        "outputId": "6d5f633c-5db1-433b-9162-9842fe519af4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "root         454     452  0 10:26 ?        00:00:00 sudo -H -u daemon elasticsearch-7.9.2/bin/elasticsearch\n",
            "daemon       455     454 99 10:26 ?        00:00:21 /content/elasticsearch-7.9.2/jdk/bin/java -Xshare:auto -Des.networkaddress.cache.ttl=60 -Des.networkaddress.cache.negative.ttl=10 -XX:+AlwaysPreTouch -Xss1m -Djava.awt.headless=true -Dfile.encoding=UTF-8 -Djna.nosys=true -XX:-OmitStackTraceInFastThrow -XX:+ShowCodeDetailsInExceptionMessages -Dio.netty.noUnsafe=true -Dio.netty.noKeySetOptimization=true -Dio.netty.recycler.maxCapacityPerThread=0 -Dio.netty.allocator.numDirectArenas=0 -Dlog4j.shutdownHookEnabled=false -Dlog4j2.disable.jmx=true -Djava.locale.providers=SPI,COMPAT -Xms1g -Xmx1g -XX:+UseG1GC -XX:G1ReservePercent=25 -XX:InitiatingHeapOccupancyPercent=30 -Djava.io.tmpdir=/tmp/elasticsearch-14036699027802730652 -XX:+HeapDumpOnOutOfMemoryError -XX:HeapDumpPath=data -XX:ErrorFile=logs/hs_err_pid%p.log -Xlog:gc*,gc+age=trace,safepoint:file=logs/gc.log:utctime,pid,tags:filecount=32,filesize=64m -XX:MaxDirectMemorySize=536870912 -Des.path.home=/content/elasticsearch-7.9.2 -Des.path.conf=/content/elasticsearch-7.9.2/config -Des.distribution.flavor=oss -Des.distribution.type=tar -Des.bundled_jdk=true -cp /content/elasticsearch-7.9.2/lib/* org.elasticsearch.bootstrap.Elasticsearch\n",
            "root         757     755  0 10:27 ?        00:00:00 grep elasticsearch\n"
          ]
        }
      ],
      "source": [
        "%%bash\n",
        "\n",
        "ps -ef | grep elasticsearch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8oBRxkdpd64Q",
        "outputId": "ec28607e-46ec-4178-e858-b0e90f27a65a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{\n",
            "  \"name\" : \"fd2a4e8b659b\",\n",
            "  \"cluster_name\" : \"elasticsearch\",\n",
            "  \"cluster_uuid\" : \"HnylE71GTH2OXuzKHzDVsA\",\n",
            "  \"version\" : {\n",
            "    \"number\" : \"7.9.2\",\n",
            "    \"build_flavor\" : \"oss\",\n",
            "    \"build_type\" : \"tar\",\n",
            "    \"build_hash\" : \"d34da0ea4a966c4e49417f2da2f244e3e97b4e6e\",\n",
            "    \"build_date\" : \"2020-09-23T00:45:33.626720Z\",\n",
            "    \"build_snapshot\" : false,\n",
            "    \"lucene_version\" : \"8.6.2\",\n",
            "    \"minimum_wire_compatibility_version\" : \"6.8.0\",\n",
            "    \"minimum_index_compatibility_version\" : \"6.0.0-beta1\"\n",
            "  },\n",
            "  \"tagline\" : \"You Know, for Search\"\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "%%bash\n",
        "\n",
        "curl -sX GET \"localhost:9200/\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LR5PftFXXyzN"
      },
      "outputs": [],
      "source": [
        "from collections import Counter\n",
        "from elasticsearch import Elasticsearch\n",
        "from pprint import pprint\n",
        "\n",
        "import ipytest\n",
        "import pytest\n",
        "\n",
        "ipytest.autoconfig()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UGHBzBQNXyzN"
      },
      "source": [
        "## Indexing a toy collection\n",
        "\n",
        "This time, we store **term position information** and perform minimal stemming, i.e., removing only plurals (for that, we specify a custom analyzer).\n",
        "\n",
        "Check the [Elasticsearch documentation on analyzers](https://www.elastic.co/guide/en/elasticsearch/reference/current/analyzer.html)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AvETQfFnXyzN"
      },
      "outputs": [],
      "source": [
        "INDEX_NAME = \"toy_index\"\n",
        "\n",
        "INDEX_SETTINGS = {\n",
        "    'settings' : {\n",
        "        'index' : {\n",
        "            \"number_of_shards\" : 1,\n",
        "            \"number_of_replicas\" : 1\n",
        "        },\n",
        "        'analysis': {\n",
        "            'analyzer': {\n",
        "                'my_english_analyzer': {\n",
        "                    'type': \"custom\",\n",
        "                    'tokenizer': \"standard\",\n",
        "                    'stopwords': \"_english_\",\n",
        "                    'filter': [\n",
        "                        \"lowercase\",\n",
        "                        \"english_stop\",\n",
        "                        \"filter_english_minimal\"\n",
        "                    ]\n",
        "                }\n",
        "            },\n",
        "            'filter' : {\n",
        "                'filter_english_minimal' : {\n",
        "                    'type': \"stemmer\",\n",
        "                    'name': \"minimal_english\"\n",
        "                },\n",
        "                'english_stop': {\n",
        "                    'type': \"stop\",\n",
        "                    'stopwords': \"_english_\"\n",
        "                }\n",
        "            },\n",
        "        }\n",
        "    },\n",
        "    'mappings': {\n",
        "        'properties': {\n",
        "            'title': {\n",
        "                'type': \"text\",\n",
        "                'term_vector': \"with_positions\",\n",
        "                'analyzer': \"my_english_analyzer\"\n",
        "            },\n",
        "            'content': {\n",
        "                'type': \"text\",\n",
        "                'term_vector': \"with_positions\",\n",
        "                'analyzer': \"my_english_analyzer\"\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nQvCcwDrXyzO"
      },
      "outputs": [],
      "source": [
        "DOCS = {\n",
        "    1: {\"title\": \"Rap God\",\n",
        "        \"content\": \"gonna, gonna, Look, I was gonna go easy on you and not to hurt your feelings\"\n",
        "        },\n",
        "    2: {\"title\": \"Lose Yourself\",\n",
        "        \"content\": \"Yo, if you could just, for one minute Or one split second in time, forget everything Everything that bothers you, or your problems Everything, and follow me\"\n",
        "        },\n",
        "    3: {\"title\": \"Love The Way You Lie\",\n",
        "        \"content\": \"Just gonna stand there and watch me burn But that's alright, because I like the way it hurts\"\n",
        "        },\n",
        "    4: {\"title\": \"The Monster\",\n",
        "        \"content\": [\"gonna gonna I'm friends with the monster\", \"That's under my bed Get along with the voices inside of my head\"]\n",
        "        },\n",
        "    5: {\"title\": \"Beautiful\",\n",
        "        \"content\": \"Lately I've been hard to reach I've been too long on my own Everybody has a private world Where they can be alone\"\n",
        "        },\n",
        "    6: {\"title\": \"Fake Eminem 1\",\n",
        "        \"content\": \"This is not real Eminem, just some text to get more matches for a split second for a split second.\"\n",
        "        },\n",
        "    7: {\"title\": \"Fake Eminem 2\",\n",
        "        \"content\": \"I have a monster friend and I'm friends with the monster and then there are some more friends who are monsters.\"\n",
        "        },\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kfPB4wdyXyzO"
      },
      "outputs": [],
      "source": [
        "ES_NODES = \"http://localhost:9200\"\n",
        "es = Elasticsearch(hosts = [ES_NODES])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ozCr_Z1ehg23",
        "outputId": "80067269-a0f6-4b68-ffa4-527e64833924"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'name': 'fd2a4e8b659b',\n",
              " 'cluster_name': 'elasticsearch',\n",
              " 'cluster_uuid': 'HnylE71GTH2OXuzKHzDVsA',\n",
              " 'version': {'number': '7.9.2',\n",
              "  'build_flavor': 'oss',\n",
              "  'build_type': 'tar',\n",
              "  'build_hash': 'd34da0ea4a966c4e49417f2da2f244e3e97b4e6e',\n",
              "  'build_date': '2020-09-23T00:45:33.626720Z',\n",
              "  'build_snapshot': False,\n",
              "  'lucene_version': '8.6.2',\n",
              "  'minimum_wire_compatibility_version': '6.8.0',\n",
              "  'minimum_index_compatibility_version': '6.0.0-beta1'},\n",
              " 'tagline': 'You Know, for Search'}"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "es.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZgYzlrcrXyzO"
      },
      "outputs": [],
      "source": [
        "if es.indices.exists(index=INDEX_NAME):\n",
        "    es.indices.delete(index=INDEX_NAME)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ILV1N_6aer2d",
        "outputId": "44468c2b-2709-4314-fa70-2dab59dcd687"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'acknowledged': True, 'shards_acknowledged': True, 'index': 'toy_index'}"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "es.indices.create(index=INDEX_NAME, body=INDEX_SETTINGS)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KHHE6EqbXyzP"
      },
      "source": [
        "Testing our analyzer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rmb5UUaOXyzP",
        "outputId": "67524c52-e538-49f4-acf4-f3afd6c2c47f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'tokens': [{'token': 'monster',\n",
              "   'start_offset': 0,\n",
              "   'end_offset': 8,\n",
              "   'type': '<ALPHANUM>',\n",
              "   'position': 0},\n",
              "  {'token': 'my',\n",
              "   'start_offset': 12,\n",
              "   'end_offset': 14,\n",
              "   'type': '<ALPHANUM>',\n",
              "   'position': 2},\n",
              "  {'token': 'bed',\n",
              "   'start_offset': 15,\n",
              "   'end_offset': 18,\n",
              "   'type': '<ALPHANUM>',\n",
              "   'position': 3}]}"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "es.indices.analyze(index=INDEX_NAME, body={'analyzer': 'my_english_analyzer', 'text': 'monsters in my bed'})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GaXc0R7pXyzP"
      },
      "outputs": [],
      "source": [
        "for doc_id, doc in DOCS.items():\n",
        "    es.index(index=INDEX_NAME, id=doc_id, body=doc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u-kQL4WpXyzP"
      },
      "source": [
        "Notice that you also get term position information when requesting a term vector."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JQJXKCQRXyzP",
        "outputId": "4c1a4e5b-d5a6-4895-d586-1313defa3392"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'_id': '2',\n",
            " '_index': 'toy_index',\n",
            " '_type': '_doc',\n",
            " '_version': 1,\n",
            " 'found': True,\n",
            " 'term_vectors': {'content': {'field_statistics': {'doc_count': 7,\n",
            "                                                   'sum_doc_freq': 85,\n",
            "                                                   'sum_ttf': 101},\n",
            "                              'terms': {'bother': {'term_freq': 1,\n",
            "                                                   'tokens': [{'position': 18}]},\n",
            "                                        'could': {'term_freq': 1,\n",
            "                                                  'tokens': [{'position': 3}]},\n",
            "                                        'everything': {'term_freq': 3,\n",
            "                                                       'tokens': [{'position': 15},\n",
            "                                                                  {'position': 16},\n",
            "                                                                  {'position': 23}]},\n",
            "                                        'follow': {'term_freq': 1,\n",
            "                                                   'tokens': [{'position': 25}]},\n",
            "                                        'forget': {'term_freq': 1,\n",
            "                                                   'tokens': [{'position': 14}]},\n",
            "                                        'just': {'term_freq': 1,\n",
            "                                                 'tokens': [{'position': 4}]},\n",
            "                                        'me': {'term_freq': 1,\n",
            "                                               'tokens': [{'position': 26}]},\n",
            "                                        'minute': {'term_freq': 1,\n",
            "                                                   'tokens': [{'position': 7}]},\n",
            "                                        'one': {'term_freq': 2,\n",
            "                                                'tokens': [{'position': 6},\n",
            "                                                           {'position': 9}]},\n",
            "                                        'problem': {'term_freq': 1,\n",
            "                                                    'tokens': [{'position': 22}]},\n",
            "                                        'second': {'term_freq': 1,\n",
            "                                                   'tokens': [{'position': 11}]},\n",
            "                                        'split': {'term_freq': 1,\n",
            "                                                  'tokens': [{'position': 10}]},\n",
            "                                        'time': {'term_freq': 1,\n",
            "                                                 'tokens': [{'position': 13}]},\n",
            "                                        'yo': {'term_freq': 1,\n",
            "                                               'tokens': [{'position': 0}]},\n",
            "                                        'you': {'term_freq': 2,\n",
            "                                                'tokens': [{'position': 2},\n",
            "                                                           {'position': 19}]},\n",
            "                                        'your': {'term_freq': 1,\n",
            "                                                 'tokens': [{'position': 21}]}}},\n",
            "                  'title': {'field_statistics': {'doc_count': 7,\n",
            "                                                 'sum_doc_freq': 16,\n",
            "                                                 'sum_ttf': 16},\n",
            "                            'terms': {'lose': {'term_freq': 1,\n",
            "                                               'tokens': [{'position': 0}]},\n",
            "                                      'yourself': {'term_freq': 1,\n",
            "                                                   'tokens': [{'position': 1}]}}}},\n",
            " 'took': 16}\n"
          ]
        }
      ],
      "source": [
        "tv = es.termvectors(index=INDEX_NAME, id=2, fields='title,content')\n",
        "pprint(tv)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aaCY3YwyXyzP"
      },
      "source": [
        "## Recovering ordered sequence of terms from inverted index\n",
        "\n",
        "This method returns the sequence of terms for a given document field, with `None` values for stopwords that got removed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zr0L-mPHXyzQ"
      },
      "outputs": [],
      "source": [
        "def get_term_sequence(es, doc_id, field):\n",
        "    tv = es.termvectors(index=INDEX_NAME, id=doc_id, fields=[field])\n",
        "    # Find max term position\n",
        "    max_term_pos = max(token['position']\n",
        "                       for _, tinfo in tv['term_vectors'][field]['terms'].items()\n",
        "                       for token in tinfo['tokens'])\n",
        "    # Recover sequence of terms\n",
        "    term_seq = [None] * (max_term_pos + 1)\n",
        "    for term, tinfo in tv['term_vectors'][field]['terms'].items():\n",
        "      for token in tinfo['tokens']:\n",
        "        term_seq[token['position']] = term\n",
        "    return term_seq"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tEV_0i6YXyzQ"
      },
      "source": [
        "Tests."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bwWlKt5nXyzQ",
        "outputId": "ea6ca1ff-859a-4a73-dbaa-1e2543227fdf"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "%%run_pytest[clean] and %%run_pytest are deprecated in favor of %%ipytest. %%ipytest will clean tests, evaluate the cell and then run pytest. To disable cleaning, configure ipytest with ipytest.config(clean=False).\n",
            "ipytest.clean_tests is deprecated in favor of ipytest.clean\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[32m.\u001b[0m\u001b[32m                                                                                            [100%]\u001b[0m\n",
            "\u001b[32m\u001b[32m\u001b[1m1 passed\u001b[0m\u001b[32m in 0.07s\u001b[0m\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "%%run_pytest[clean]\n",
        "\n",
        "def test_get_term_sequence():\n",
        "    assert get_term_sequence(es, 4, 'title') == [None, 'monster']\n",
        "    assert get_term_sequence(es, 7, 'content') == ['i', 'have', None, 'monster', 'friend', None, \"i'm\", 'friend', None, None, 'monster', None, None, None, None, 'some', 'more', 'friend', 'who', None, 'monster']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o_nG_1EUXyzQ"
      },
      "source": [
        "## Getting ordered bigram matches\n",
        "\n",
        "Use the `get_term_sequence()` method to get the document field's content as a sequence of terms, then check for ordered bigram matches yourself."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c7Q20RIdXyzQ"
      },
      "outputs": [],
      "source": [
        "def count_ordered_bigram_matches(es, doc_id, field, bigram):\n",
        "    \"\"\"Counts the number of ordered bigram matches in a given document field.\n",
        "\n",
        "    Args:\n",
        "        es: Elasticsearch instance\n",
        "        doc_id: Document ID\n",
        "        field: Document field\n",
        "        bigram: A sequence of two terms given as a list\n",
        "\n",
        "    Returns:\n",
        "        Number of times the bigram can be found in this exact order.\n",
        "    \"\"\"\n",
        "    # Get the document field's content as a sequence of terms.\n",
        "    text = get_term_sequence(es, doc_id, field)\n",
        "    # Count the number of matches\n",
        "    count = 0\n",
        "    for i in range(len(text) - 1):\n",
        "        if text[i] == bigram[0]:\n",
        "            if text[i + 1] == bigram[1]:\n",
        "                count += 1\n",
        "    return count"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eko0CqLfXyzQ"
      },
      "source": [
        "Tests."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m3AfABhmXyzQ",
        "outputId": "983f4c2c-d507-4d18-b680-45ca1de5d125"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m                                                                                          [100%]\u001b[0m\n",
            "\u001b[32m\u001b[32m\u001b[1m3 passed\u001b[0m\u001b[32m in 0.05s\u001b[0m\u001b[0m\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "%%run_pytest[clean] and %%run_pytest are deprecated in favor of %%ipytest. %%ipytest will clean tests, evaluate the cell and then run pytest. To disable cleaning, configure ipytest with ipytest.config(clean=False).\n",
            "ipytest.clean_tests is deprecated in favor of ipytest.clean\n"
          ]
        }
      ],
      "source": [
        "%%run_pytest[clean]\n",
        "\n",
        "@pytest.mark.parametrize('doc_id, field, bigram, correct_value', [\n",
        "    (6, 'content', ['split', 'second'], 2),\n",
        "    (2, 'content', ['split', 'second'], 1),\n",
        "    (1, 'content', ['split', 'second'], 0),\n",
        "])\n",
        "def test_count_ordered_bigram_matches(doc_id, field, bigram, correct_value):\n",
        "    assert count_ordered_bigram_matches(es, doc_id, field, bigram) == correct_value"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AiFiq-9JXyzQ"
      },
      "source": [
        "## Getting unordered bigram matches\n",
        "\n",
        "As before, use the `get_term_sequence()` method to get the document field's content as a sequence of terms, then check for ordered bigram matches yourself."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X0NKDqCpXyzQ"
      },
      "outputs": [],
      "source": [
        "def count_unordered_bigram_matches(es, doc_id, field, bigram, w=4):\n",
        "    \"\"\"Counts the number of unordered bigram matches in a given document field.\n",
        "\n",
        "    Args:\n",
        "        es: Elasticsearch instance\n",
        "        doc_id: Document ID\n",
        "        field: Document field\n",
        "        bigram: A sequence of two terms given as a list\n",
        "        w: The maximum distance between the two query terms that still counts as a match\n",
        "\n",
        "    Returns:\n",
        "        Number of times the bigram can be found within a distance of w from each other in any order.\n",
        "    \"\"\"\n",
        "    text = get_term_sequence(es, doc_id, \"content\")\n",
        "    count = 0\n",
        "    for i in range(len(text) - 1):\n",
        "        if text[i] in bigram:\n",
        "            other_term = bigram[0] if text[i] == bigram[1] else bigram[1]\n",
        "            count += Counter(text[i+1:i+w])[other_term]\n",
        "    return count"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RKnWleAUXyzR"
      },
      "source": [
        "Tests."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9-2SygNxXyzR",
        "outputId": "9341cf05-0076-4657-bbc0-b850d8571a48"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m                                                                                          [100%]\u001b[0m\n",
            "\u001b[32m\u001b[32m\u001b[1m3 passed\u001b[0m\u001b[32m in 0.04s\u001b[0m\u001b[0m\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "%%run_pytest[clean] and %%run_pytest are deprecated in favor of %%ipytest. %%ipytest will clean tests, evaluate the cell and then run pytest. To disable cleaning, configure ipytest with ipytest.config(clean=False).\n",
            "ipytest.clean_tests is deprecated in favor of ipytest.clean\n"
          ]
        }
      ],
      "source": [
        "%%run_pytest[clean]\n",
        "\n",
        "@pytest.mark.parametrize('doc_id, field, bigram, correct_value', [\n",
        "    (7, 'title', ['friend', 'monster'], 3),\n",
        "    (4, 'title', ['friend', 'monster'], 1),\n",
        "    (1, 'title', ['friend', 'monster'], 0),\n",
        "])\n",
        "def test_count_ordered_bigram_matches(doc_id, field, bigram, correct_value):\n",
        "    assert count_unordered_bigram_matches(es, doc_id, field, bigram) == correct_value"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
