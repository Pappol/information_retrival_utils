{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ibWxbNBhIXOh"
      },
      "source": [
        "# Entity linking evaluation\n",
        "You are provided with the documents annotations along with ground truth annotations and asked to evaluate them."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qbd9N7s5IiLM"
      },
      "outputs": [],
      "source": [
        "import ipytest\n",
        "import pytest\n",
        "\n",
        "ipytest.autoconfig()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8nu_VkIVgjnv"
      },
      "source": [
        "The annotations given by a entity linking system under evaluation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KL6yvSIcOkUU"
      },
      "outputs": [],
      "source": [
        "LINKED_ENTITIES_1 = [\n",
        "    (0, 'angola', 'wikipedia:Angola'),\n",
        "    (14, 'multiparty democracy', 'wikipedia:multiparty_democracy'),\n",
        "    (18, '1992 elections', 'wikipedia:Philippine_general_election,_1992')\n",
        "]\n",
        "\n",
        "LINKED_ENTITIES_2 = [\n",
        "    (5, 'angola', 'wikipedia:Angola'),\n",
        "    (10, '1975', 'wikipedia:Philippine_general_election,_1992'),\n",
        "    (13, 'one party', 'wikipedia:Single-party_state')\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gHVEsm3hgsCQ"
      },
      "source": [
        "Ground truth annotations (reference annotations)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4q_5Ij3fQWR0"
      },
      "outputs": [],
      "source": [
        "GROUND_TRUTH_ANNOTATIONS_1 = [\n",
        "    (0, 'angola', 'wikipedia:Angola'),\n",
        "    (4, 'one-party', 'wikipedia:Single-party_state'),\n",
        "    (14, 'multiparty democracy', 'wikipedia:multiparty_democracy'),\n",
        "    (18, '1992 elections', 'wikipedia:Philippine_general_election,_1992')\n",
        "]\n",
        "\n",
        "GROUND_TRUTH_ANNOTATIONS_2 = [\n",
        "    (5, 'angola', 'wikipedia:Angola'),\n",
        "    (13, 'one party', 'wikipedia:Single-party_state'),\n",
        "    (14, 'Republic', 'wikipedia:Republic')\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DOsTzvL_g1e7"
      },
      "source": [
        "Set-based metrics where:\n",
        "- precision is defined as the fraction of correctly linked entities that have been annotated by the system\n",
        "- recall is defined as fraction of correctly linked entities that should be annotated\n",
        "- F-measure is a harmonic mean between precision and recall"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZMDYNJVZX6Hs"
      },
      "outputs": [],
      "source": [
        "def set_based_precision(annotations, relevance_annotations):\n",
        "  \"\"\"Computes set-based precision.\n",
        "\n",
        "  Args:\n",
        "      annotations: All annotations for a set of documents.\n",
        "      relevance_annotations: All reference (ground truth) annotations for a set of documents.\n",
        "\n",
        "  Returns:\n",
        "      Set-based precision.\n",
        "  \"\"\"\n",
        "  return len(set(annotations).intersection(relevance_annotations))/len(annotations)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YHITcKHIYK6r"
      },
      "outputs": [],
      "source": [
        "def set_based_recall(annotations, relevance_annotations):\n",
        "  \"\"\"Computes set-based recall.\n",
        "\n",
        "  Args:\n",
        "      annotations: All annotations for a set of documents.\n",
        "      relevance_annotations: All reference (ground truth) annotations for a set of documents.\n",
        "\n",
        "  Returns:\n",
        "      Set-based recall.\n",
        "  \"\"\"\n",
        "  return len(set(annotations).intersection(relevance_annotations))/len(relevance_annotations)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qWRO31XBdtcy"
      },
      "outputs": [],
      "source": [
        "def f1_score(precision, recall):\n",
        "  \"\"\"Computes F-measure.\n",
        "\n",
        "  Args:\n",
        "      annotations: All annotations for a set of documents.\n",
        "      relevance_annotations: All reference (ground truth) annotations for a set of documents.\n",
        "\n",
        "  Returns:\n",
        "      F-measure.\n",
        "  \"\"\"\n",
        "  return 2 * precision * recall / (precision + recall)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tcBzUihbhb_j"
      },
      "source": [
        "## Metrics over the collection of documents\n",
        "\n",
        "Micro-averaged - averaged across mentions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hpSNh02zQ29O"
      },
      "outputs": [],
      "source": [
        "import itertools\n",
        "\n",
        "def micro_precision(annotations, ground_truth_annotations):\n",
        "  \"\"\"Computes micro-averaged precision.\n",
        "\n",
        "  Args:\n",
        "      annotations: All annotations for a set of documents.\n",
        "      relevance_annotations: All reference (ground truth) annotations for a set of documents.\n",
        "\n",
        "  Returns:\n",
        "      Micro-averaged precision.\n",
        "  \"\"\"\n",
        "  all_annotations = list(itertools.chain(*annotations))\n",
        "  all_ground_truth_annotations = list(itertools.chain(*ground_truth_annotations))\n",
        "  return set_based_precision(all_annotations, all_ground_truth_annotations)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KoKdOBzhSta7"
      },
      "outputs": [],
      "source": [
        "def micro_recall(all_annotations, ground_truth_annotations):\n",
        "  \"\"\"Computes micro-averaged recall.\n",
        "\n",
        "  Args:\n",
        "      annotations: All annotations for a set of documents.\n",
        "      relevance_annotations: All reference (ground truth) annotations for a set of documents.\n",
        "\n",
        "  Returns:\n",
        "      Micro-averaged recall.\n",
        "  \"\"\"\n",
        "  all_annotations = list(itertools.chain(*all_annotations))\n",
        "  all_ground_truth_annotations = list(itertools.chain(*ground_truth_annotations))\n",
        "  return set_based_recall(all_annotations, all_ground_truth_annotations)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Np0FExe9TSpH"
      },
      "source": [
        "Tests"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f78_TzYnTSUV"
      },
      "outputs": [],
      "source": [
        "%%run_pytest[clean]\n",
        "\n",
        "def test_micro_precision():\n",
        "  assert micro_precision([LINKED_ENTITIES_1, LINKED_ENTITIES_2], [GROUND_TRUTH_ANNOTATIONS_1, GROUND_TRUTH_ANNOTATIONS_2]) == pytest.approx(5/6, rel=1e-2)\n",
        "\n",
        "def test_micro_recall():\n",
        "  assert micro_recall([LINKED_ENTITIES_1, LINKED_ENTITIES_2], [GROUND_TRUTH_ANNOTATIONS_1, GROUND_TRUTH_ANNOTATIONS_2]) == pytest.approx(5/7, rel=1e-2)\n",
        "\n",
        "def test_micro_f1():\n",
        "  micro_p = micro_precision([LINKED_ENTITIES_1, LINKED_ENTITIES_2], [GROUND_TRUTH_ANNOTATIONS_1, GROUND_TRUTH_ANNOTATIONS_2])\n",
        "  micro_r = micro_recall([LINKED_ENTITIES_1, LINKED_ENTITIES_2], [GROUND_TRUTH_ANNOTATIONS_1, GROUND_TRUTH_ANNOTATIONS_2])\n",
        "  assert f1_score(micro_p, micro_r) == pytest.approx((2 * 5/6 * 5/7) / (5/6 + 5/7), rel=1e-2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6vNQN5oIhl7x"
      },
      "source": [
        "Macro-averaged - averaged across documents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0lYkqs2dXG0c"
      },
      "outputs": [],
      "source": [
        "def macro_precision(annotations, ground_truth_annotations):\n",
        "  \"\"\"Computes macro-averaged precision.\n",
        "\n",
        "  Args:\n",
        "      annotations: All annotations for a set of documents.\n",
        "      relevance_annotations: All reference (ground truth) annotations for a set of documents.\n",
        "\n",
        "  Returns:\n",
        "      Macro-averaged precision.\n",
        "  \"\"\"\n",
        "  return sum(set_based_precision(annotation, ground_truth) for annotation, ground_truth\n",
        "             in zip(annotations, ground_truth_annotations))/len(ground_truth_annotations)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KyXXm-_1YuKO"
      },
      "outputs": [],
      "source": [
        "def macro_recall(annotations, ground_truth_annotations):\n",
        "  \"\"\"Computes macro-averaged recall.\n",
        "\n",
        "  Args:\n",
        "      annotations: All annotations for a set of documents.\n",
        "      relevance_annotations: All reference (ground truth) annotations for a set of documents.\n",
        "\n",
        "  Returns:\n",
        "      Macro-averaged recall.\n",
        "  \"\"\"\n",
        "  return sum(set_based_recall(annotation, ground_truth) for annotation, ground_truth\n",
        "             in zip(annotations, ground_truth_annotations))/len(ground_truth_annotations)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YCkmcF6_hqP1"
      },
      "source": [
        "Tests"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rymX_XcYYy5n"
      },
      "outputs": [],
      "source": [
        "%%run_pytest[clean]\n",
        "\n",
        "def test_macro_precision():\n",
        "  assert macro_precision([LINKED_ENTITIES_1, LINKED_ENTITIES_2], [GROUND_TRUTH_ANNOTATIONS_1, GROUND_TRUTH_ANNOTATIONS_2]) == pytest.approx((1 + 2/3)/2, rel=1e-2)\n",
        "\n",
        "def test_macro_recall():\n",
        "  assert macro_recall([LINKED_ENTITIES_1, LINKED_ENTITIES_2], [GROUND_TRUTH_ANNOTATIONS_1, GROUND_TRUTH_ANNOTATIONS_2]) == pytest.approx((3/4 + 2/3)/2, rel=1e-2)\n",
        "\n",
        "def test_macro_f1():\n",
        "  macro_p = macro_precision([LINKED_ENTITIES_1, LINKED_ENTITIES_2], [GROUND_TRUTH_ANNOTATIONS_1, GROUND_TRUTH_ANNOTATIONS_2])\n",
        "  macro_r = macro_recall([LINKED_ENTITIES_1, LINKED_ENTITIES_2], [GROUND_TRUTH_ANNOTATIONS_1, GROUND_TRUTH_ANNOTATIONS_2])\n",
        "  assert f1_score(macro_p, macro_r) == pytest.approx((2 * 5/6 * 17/24) / (5/6 + 17/24), rel=1e-2)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
