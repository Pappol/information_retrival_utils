{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ceYNFACNzYzX"
      },
      "source": [
        "# Naive Bayes text classifier\n",
        "\n",
        "In this exercise, you'll implement a Naive Bayes classifier for text from scratch."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install ipytest"
      ],
      "metadata": {
        "id": "7DlB0nYezxyG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "58d8e69c-5a44-405d-888d-0ab6760eda09"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ipytest\n",
            "  Downloading ipytest-0.13.3-py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: ipython in /usr/local/lib/python3.10/dist-packages (from ipytest) (7.34.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from ipytest) (23.1)\n",
            "Requirement already satisfied: pytest>=5.4 in /usr/local/lib/python3.10/dist-packages (from ipytest) (7.4.0)\n",
            "Requirement already satisfied: iniconfig in /usr/local/lib/python3.10/dist-packages (from pytest>=5.4->ipytest) (2.0.0)\n",
            "Requirement already satisfied: pluggy<2.0,>=0.12 in /usr/local/lib/python3.10/dist-packages (from pytest>=5.4->ipytest) (1.2.0)\n",
            "Requirement already satisfied: exceptiongroup>=1.0.0rc8 in /usr/local/lib/python3.10/dist-packages (from pytest>=5.4->ipytest) (1.1.3)\n",
            "Requirement already satisfied: tomli>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from pytest>=5.4->ipytest) (2.0.1)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.10/dist-packages (from ipython->ipytest) (67.7.2)\n",
            "Collecting jedi>=0.16 (from ipython->ipytest)\n",
            "  Downloading jedi-0.19.0-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m18.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from ipython->ipytest) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.10/dist-packages (from ipython->ipytest) (0.7.5)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.10/dist-packages (from ipython->ipytest) (5.7.1)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from ipython->ipytest) (3.0.39)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.10/dist-packages (from ipython->ipytest) (2.16.1)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.10/dist-packages (from ipython->ipytest) (0.2.0)\n",
            "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.10/dist-packages (from ipython->ipytest) (0.1.6)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.10/dist-packages (from ipython->ipytest) (4.8.0)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.3 in /usr/local/lib/python3.10/dist-packages (from jedi>=0.16->ipython->ipytest) (0.8.3)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.10/dist-packages (from pexpect>4.3->ipython->ipytest) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython->ipytest) (0.2.6)\n",
            "Installing collected packages: jedi, ipytest\n",
            "Successfully installed ipytest-0.13.3 jedi-0.19.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EsGmC71KzYzg"
      },
      "outputs": [],
      "source": [
        "import ipytest\n",
        "from typing import List\n",
        "\n",
        "ipytest.autoconfig()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fbn-FoRpzYzl"
      },
      "source": [
        "### Training the model\n",
        "\n",
        "  - Calculate $P(y)$ for each class label in the training data\n",
        "  - Calculate $P(x_i|y)$ for each feature (term) for each class label in the training data using Laplace (add-one) smoothing\n",
        "  \n",
        "$$P(x_i|y)=\\frac{c_{i,y} + 1}{c_y + m}$$\n",
        "\n",
        "where\n",
        "  - $c_{i,y}$ is the number of times term $x_i$ appears in class $y$\n",
        "  - $c_y$ is the total number of term occurrences in class $y$\n",
        "  - $m$ is the size of the vocabulary\n",
        "\n",
        "\n",
        "### Applying the model\n",
        "\n",
        "Return the class $y \\in Y$ that maximizes $P(y) \\prod_{x_i} P(x_i|y)$.\n",
        "\n",
        "Note that we need to consider $x_i$ at each *word position* in the document. Thus, we need to multiply with $P(x_i|y)$ as many times as $x_i$ appears in the document.\n",
        "We can rewrite it as: $$P(y|x) \\propto P(y) \\prod_{i \\in d} P(x_i|y)^{c_{i,d}}$$ where $c_{i,d}$ is the number of times term $i$ appears in document $d$.\n",
        "\n",
        "Finally, we perform the computations in the log domain, that is, $$\\log P(y) +  \\sum_{i=1}^n (c_{i,d} \\log P(x_i|y))$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gESAyM8czYzl"
      },
      "source": [
        "## 1) Probability estimations\n",
        "\n",
        "The estimation of probabilities $P(x_i|y)$ and $P(y)$ are refactored to a separate class to make them testable."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DOuxB-qYzYzm"
      },
      "outputs": [],
      "source": [
        "class NBProbabilityEstimator:\n",
        "\n",
        "    def get_prior_prob(self, y: int, training_labels: List[int]) -> float:\n",
        "        \"\"\"Computes the class prior probability, P(y).\n",
        "\n",
        "        Args:\n",
        "            y: Class ID.\n",
        "            training_labels: Class labels in training data.\n",
        "\n",
        "        Returns:\n",
        "            The probability P(y).\n",
        "        \"\"\"\n",
        "        return training_labels.count(y) / len(training_labels)\n",
        "\n",
        "    def get_term_prob(self,\n",
        "                      count_term_in_class: int,\n",
        "                      count_all_terms_in_class: int,\n",
        "                      num_terms: int) -> float:\n",
        "        \"\"\"Computes the smoothed term probability for a given class, P(x_i|y).\n",
        "\n",
        "        Args:\n",
        "          count_term_in_class: Number of times the term appears in the given class.\n",
        "          count_all_terms_in_class: Total number of term occurrences in class.\n",
        "          num_terms: Size of the vocabulary.\n",
        "\n",
        "        Returns:\n",
        "          The probability P(x_i|y).\n",
        "        \"\"\"\n",
        "        return (count_term_in_class + 1) / (count_all_terms_in_class + num_terms)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gMOzJYpszYzp"
      },
      "source": [
        "### Tests"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eUQLB1wezYzq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "62899d82-2496-47d5-b1b8-f8d4d8912344"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m                                                                                           [100%]\u001b[0m\n",
            "\u001b[32m\u001b[32m\u001b[1m2 passed\u001b[0m\u001b[32m in 0.02s\u001b[0m\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "%%run_pytest[clean] and %%run_pytest are deprecated in favor of %%ipytest. %%ipytest will clean tests, evaluate the cell and then run pytest. To disable cleaning, configure ipytest with ipytest.config(clean=False).\n",
            "ipytest.clean_tests is deprecated in favor of ipytest.clean\n"
          ]
        }
      ],
      "source": [
        "%%run_pytest[clean]\n",
        "\n",
        "def test_prior_prob():\n",
        "    nbpe = NBProbabilityEstimator()\n",
        "    assert nbpe.get_prior_prob(1, [0, 1, 2, 3]) == 0.25\n",
        "    assert nbpe.get_prior_prob(1, [1, 1, 2, 3]) == 0.5\n",
        "\n",
        "def test_term_prob():\n",
        "    nbpe = NBProbabilityEstimator()\n",
        "    assert nbpe.get_term_prob(5, 20, 10) == 0.2\n",
        "    assert nbpe.get_term_prob(74, 90, 10) == 0.75\n",
        "    assert nbpe.get_term_prob(0, 6, 10) == 0.0625"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y1swdMcezYz2"
      },
      "source": [
        "## 2) Naive Bayes classifier\n",
        "\n",
        "Implement training and prediction for a Naive Bayes classifier.  We are operating with dense matrices for simplicity."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z5hK0VMFzYz3"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import math\n",
        "\n",
        "class NBClassifier:\n",
        "\n",
        "    def __init__(self) -> None:\n",
        "        self._nbprob = NBProbabilityEstimator()\n",
        "        self._num_classes = 0\n",
        "        self._prior_prob = None  # Holds P(y) values\n",
        "        self._term_prob = None  # Holds P(x_i|y) values\n",
        "\n",
        "\n",
        "    def fit(self, X_train: List[List[int]], y_train: List[int]) -> None:\n",
        "        \"\"\"Fits the model.\n",
        "\n",
        "        Args:\n",
        "          X_train: Document-term matrix for training data.\n",
        "              Rows correspond to documents and columns correspond to terms.\n",
        "          y_train: Class labels corresponding to training documents.\n",
        "        \"\"\"\n",
        "        self._num_classes = len(np.unique(y_train))\n",
        "        num_docs = len(X_train)\n",
        "        num_terms = len(X_train[0])\n",
        "        self._term_prob = np.zeros((num_terms, self._num_classes))\n",
        "\n",
        "        # Calculate c_y values, i.e., the total number of term occurrences in each class.\n",
        "        count_all_terms_in_class = [0] * self._num_classes\n",
        "        for d in range(num_docs):\n",
        "          count_all_terms_in_class[y_train[d]] += sum(X_train[d])\n",
        "\n",
        "        # Iterating through the vocabulary\n",
        "        for i in range(num_terms):\n",
        "            # Holds c_{i,y} values, i.e., the number of times term i appears with class y.\n",
        "            count_term_in_class = [0] * self._num_classes\n",
        "            for d in range(num_docs):\n",
        "                count_term_in_class[y_train[d]] += X_train[d][i]\n",
        "\n",
        "            # Calculate P(x_i|y)\n",
        "            for y in range(self._num_classes):\n",
        "                self._term_prob[i, y] = self._nbprob.get_term_prob(\n",
        "                    count_term_in_class[y],\n",
        "                    count_all_terms_in_class[y],\n",
        "                    num_terms)\n",
        "\n",
        "        # Pre-compute class prior probabilities\n",
        "        self._prior_prob = []\n",
        "        for y in range(self._num_classes):\n",
        "            self._prior_prob.append(self._nbprob.get_prior_prob(y, y_train))\n",
        "\n",
        "\n",
        "    def _predict_instance(self, x: List[int]) -> int:\n",
        "        \"\"\"Predict class for a single instance (document).\n",
        "\n",
        "        Args:\n",
        "          x: Document term vector.\n",
        "\n",
        "        Returns:\n",
        "          The predicted class label (0-indexed).\n",
        "        \"\"\"\n",
        "        probs = []\n",
        "\n",
        "        for y in range(self._num_classes):\n",
        "            p = math.log(self._prior_prob[y])\n",
        "            for i in range(len(x)):\n",
        "                if x[i] > 0:\n",
        "                    p += x[i] * math.log(self._term_prob[i][y])\n",
        "            probs.append(p)\n",
        "\n",
        "        # Get the class with the highest probability.\n",
        "        return probs.index(max(probs))\n",
        "\n",
        "\n",
        "    def predict(self, X_test: List[List[int]]) -> List[float]:\n",
        "        \"\"\"Make predictions for a set of documents.\n",
        "\n",
        "        Args:\n",
        "          X_test: Document-term matrix for test data.\n",
        "\n",
        "        Returns:\n",
        "          List with predictions.\n",
        "        \"\"\"\n",
        "        return [self._predict_instance(x) for x in X_test]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nA1xiVXizYz6"
      },
      "source": [
        "## 3) Testing on real data\n",
        "\n",
        "We will be using a subset of the 20Newsgroups collection."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sN3Rb5XdzYz8"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import fetch_20newsgroups\n",
        "\n",
        "categories = [\n",
        "    \"alt.atheism\",\n",
        "    \"soc.religion.christian\",\n",
        "    \"talk.religion.misc\",\n",
        "    \"comp.sys.ibm.pc.hardware\",\n",
        "    \"comp.sys.mac.hardware\"\n",
        "]\n",
        "\n",
        "train = fetch_20newsgroups(subset='train', categories=categories, shuffle=True, random_state=123)\n",
        "test = fetch_20newsgroups(subset='test', categories=categories, shuffle=True, random_state=123)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g5_16ZTWzYz8"
      },
      "source": [
        "### Feature extraction\n",
        "\n",
        "Get term frequencies using `CountVectorizer`. (We ignore terms that appear in less than 10 documents to speed up computation.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZDv9IBm4zYz8"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "count_vect = CountVectorizer(min_df=10)  # Note: Removing min_df will yield a slower model with better accuracy\n",
        "X_train_counts = count_vect.fit_transform(train.data)\n",
        "X_test_counts = count_vect.transform(test.data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mal71SvQzYz9"
      },
      "source": [
        "### Train and apply model\n",
        "\n",
        "Note that we convert sparse matrices to dense ones. This is not efficient and should be avoided when working with large datasets. Nevertheless, this simplifies the implementation for this exercise."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f8OW0YIGzYz-"
      },
      "outputs": [],
      "source": [
        "nb = NBClassifier()\n",
        "nb.fit(X_train_counts.toarray(), train.target.tolist())\n",
        "predicted = nb.predict(X_test_counts.toarray())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bY6t-QhIzY0A"
      },
      "source": [
        "### Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SxahB86tzY0A",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "68c16d4c-533e-4853-8604-63971ba767a1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.838\n"
          ]
        }
      ],
      "source": [
        "from sklearn import metrics\n",
        "\n",
        "print(f\"{metrics.accuracy_score(test.target, np.asarray(predicted)):.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-sMrq5qFzY0A"
      },
      "source": [
        "## Optional exercises"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O9n9Ai-JzY0B"
      },
      "source": [
        "If you're done, try to implement it without making a conversion to dense matrices.\n",
        "\n",
        "Also, do we really need to precompute and store all term probabilities?"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}